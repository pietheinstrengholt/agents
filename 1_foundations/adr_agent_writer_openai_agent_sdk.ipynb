{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "from agents import Agent, Runner, function_tool, OpenAIChatCompletionsModel, set_tracing_disabled\n",
    "set_tracing_disabled(disabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AsyncAzureOpenAI, OpenAIError\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
    "\n",
    "# Initialize Azure OpenAI client with Entra ID authentication\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(),\n",
    "    \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "openai_client = AsyncAzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    azure_ad_token_provider=token_provider,\n",
    "    api_version=\"2025-01-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"files/WS2_Architecture_proposal.pdf\")\n",
    "architecture_proposal = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        architecture_proposal += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revision History\n",
      "Version Revision date Status Summary of changes By\n",
      "0.1 Concept Initial version Piethein Strengholt\n",
      "0.2 26-05-2026 Concept Second version Discussed with Rob Visser, Tjerrie Smit, Sonja Miljoen, John van Iersel, Arno Goosen\n",
      "0.2 26-05-2026 Concept Incorporating feedback from PWC and International stakeholders Informed: Bob Zondervan, Adrian Matei, Imre Sztanó, Frank Eijsink\n",
      "0.3 12-06-2025 Concept Revised different scenarios, added extra requirements Review Jack Jonathans\n",
      "0.4 16-06-2025 For information Discussed with McKinsey, Katalin Tatar\n",
      "0.5 26-06-2025 Review Feeback WS2 working group Jack Jonathans, Marcel van Dijk, Arno Goosen, Sonja Miljoen, Hein Peters\n",
      "0.6 26-06-2025 For information Informed: Wouter Wijnmalen, Maarten de Regt, Laurens van Beurden, Jorik Blaas-Sigmond\n",
      "0.7 04-07-2025 Discussion Discussed with Tjeerd Bosklopper, Maurice Koopman, Guido Bosch, Rob Visser, Tjerrie Smit\n",
      "0.8 16-07-2025 Discussion Changes for TPAB Discussed within TPAB\n",
      "Document ManagementMission Statement for NN's Data Strategy\n",
      "Aspiration\n",
      "2\n",
      "Our North Star is to be a digitally empowered organization that places data at the heart \n",
      "of our decision-making processes\n",
      "• NN is committed to be front runner on the data & ai insurance market.\n",
      "• International, for example, is seeing the potential of at least €150 mil. with Agentic AI\n",
      "Current \n",
      "situation\n",
      "• Inspired by the Seattle trip, NN is dedicated to embark on the Agentic AI journey.\n",
      "• Non-life is ready to engage now with Quantum Black for AI use cases for claims and \n",
      "underwriting.\n",
      "• NN International is ready to engage now with PWC for the data platform and Salesforce & \n",
      "Accenture for the customer service use cases.\n",
      "• AI is evolving at an incredible speed. Scaling the current architecture is challenging.\n",
      "Recommendation\n",
      "• We need to establish a solid foundation for scaling AI throughout the organization.\n",
      "• We want to learn from our mistakes, like diversity of different NNDAP platforms. With \n",
      "strong discipline at the core, we would like to do it right from the beginning.Current landscape\n",
      "3\n",
      "Abstract view of our today’s architecture\n",
      "User interfaces Reports\n",
      "SQL\n",
      "Analytical platforms\n",
      "(structured data only)\n",
      "Training\n",
      "Process- or  \n",
      "application layer\n",
      "(business logic and orchestration)\n",
      "APIs (CRUD)\n",
      "Operational systems\n",
      "ML\n",
      "Runtime\n",
      "Current situation:\n",
      "Varied human-based experiences: Today, the interaction is often \n",
      "characterized by a traditional approach, where end-users \n",
      "predominantly rely on interfaces rather than autonomous \n",
      "systems. The engagement is typically supportive, involving the \n",
      "generation of reports derived from the analytical chain.\n",
      "Process orchestration: This is generally rule-based and closely \n",
      "integrated with applications and software systems.\n",
      "Analytical data: Platforms typically concentrate on structured, \n",
      "batch-driven data.\n",
      "Machine Learning relies on analytical systems for data processing \n",
      "and training. These systems play a crucial role in preparing and \n",
      "refining the data needed for effective model development. \n",
      "However, the operationalization of the analytical models primarily \n",
      "occurs on the operational side.How will applications be transformed with Agentic AI?\n",
      "4\n",
      "Agentic AI refers to an artificial intelligence system designed to achieve specific goals with minimal human \n",
      "supervision\n",
      "The impact of artificial intelligence (AI) on modern \n",
      "applications is profound and multifaceted, transforming \n",
      "the way software interacts with users and processes \n",
      "information. As AI continue to evolve, they are \n",
      "increasingly integrated into various applications, \n",
      "enhancing their capabilities and creating new \n",
      "opportunities for innovation.\n",
      "At the core of this transformation is the concept of AI \n",
      "agents—autonomous or semi-autonomous entities that \n",
      "can perform tasks, make decisions, and learn from their \n",
      "environment. These agents work with Large Language \n",
      "Models, retrieve data, and can be orchestrated through \n",
      "frameworks like LangChain, LangGraph, and CrewAI, \n",
      "which enable developers to create sophisticated \n",
      "applications that leverage the power of AI.\n",
      "Agent \n",
      "orchestration\n",
      "Application\n",
      "Context\n",
      "LLM\n",
      "Tools\n",
      "Agent\n",
      "Agent\n",
      "Agent\n",
      "Frameworks like \n",
      "LangChain, LangGraph, \n",
      "CrewAI\n",
      "Orchestration and decisioningFuture state: how AI is impacting the overall landscape\n",
      "5\n",
      "Abstract view of our future architecture\n",
      "Future situation:\n",
      "Empowering users with contextual intelligence: Leverage contextual data to \n",
      "deliver actionable insights and recommendations, transforming user \n",
      "experiences and driving productivity.\n",
      "Agentic Intelligence: Move beyond rule-based automation to contextually \n",
      "aware systems that adapt to real-time changes, enhancing operational \n",
      "efficiency.\n",
      "Context-Aware Interactions: Integrate context-aware protocols like the Model \n",
      "Context Protocol (MCP) to enable applications that intelligently respond.\n",
      "Unstructured data: AI's ability to process both structured and unstructured \n",
      "data (documents, log files, images, videos, and so on) for richer insights, driving \n",
      "informed decision-making across the organization. \n",
      "Generative AI is poised to infiltrate both operational and analytical side of the architecture. \n",
      "Consequently, agents must be deployed different environments. \n",
      "To fully leverage the capabilities of AI, it is essential that operational systems are exposed \n",
      "through modern APIs. Furthermore, a careful review and redefinition of business processes is \n",
      "essential to ensure that organizations can effectively harness the power of Agentic AI.\n",
      "Lastly, without reliable and well-structured data, the effectiveness of AI applications can be \n",
      "significantly diminished.\n",
      "Conversational layer\n",
      "(interfaces for voice, web, search, instructions, video, etc.) Gen BI\n",
      "Context-aware layer\n",
      "APIs (CRUD)\n",
      "Operational systems\n",
      "ML Runtime\n",
      "Context-aware layer\n",
      "Analytical platforms\n",
      "(structured + unstructured)\n",
      "SQLGenAITraining\n",
      "Agentic layer\n",
      "(query resources, execute tasks, perform reflections, orchestrate, etc.)\n",
      "Agent runtime Agent runtimeWhere are we today?\n",
      "• NNDAP v4.x Adoption Issues: The current version of NNDAP v4.x lacks broad adoption. A move to a layered \n",
      "architecture (minimal platform core L1, expanded platform L2, and extended platform L3) is necessary, \n",
      "requiring migrations to NNDAP v5.x L1.\n",
      "• Lack of ODS and Unstructured Data Support: NNDAP does not support Operational Data Store (ODS) \n",
      "functionality and lacks support for unstructured data, both of which need architectural decisions for \n",
      "resolution.\n",
      "• NNDIP Adoption Challenges: NNDIP has not been widely adopted, potentially due to the strong focus on \n",
      "MLflow and lack of self-service and automation.\n",
      "• GAIA Architecture: The GAIA architecture lacks clear definitions and product management.\n",
      "• Collibra and Unity Catalog Issues: The issues of Collibra and Unity Catalog large stem from a lack of \n",
      "collaboration within NN and insufficient knowledge across the team. Moreover, we lack the skills to set up \n",
      "platforms effectively at an enterprise level, focusing too much on minor differences rather than \n",
      "commonalities.\n",
      "• Need for AI Governance Replacement: The SECA for AI Governance is deemed insufficient and requires \n",
      "replacement with better solutions like Collibra, Monitaur, or ServiceNow AI Control Tower.\n",
      "6\n",
      "Enterprise architecture should have \n",
      "a stronger leading voice in setting \n",
      "the direction for the future Data & \n",
      "AI architecture. The current \n",
      "capabilities are mostly developed \n",
      "apart from the other capabilities.\n",
      "A comprehensive end-to-end \n",
      "architecture is lacking, which is \n",
      "hindering efficient alignment \n",
      "across different teams.\n",
      "Diverse technology stack and varied maturity levels are hindering NN’s scaling ambitionsGAIA\n",
      "Break down of the current-state data architecture\n",
      "7\n",
      "Source system\n",
      "Integration layer\n",
      "API Gateway\n",
      " Event Streaming (work-in-progress)\n",
      "Operational Data Stores\n",
      "Governance Layer\n",
      "NNDAP\n",
      "Lakehouse \n",
      "(Medallion design with Delta)\n",
      "Spark \n",
      "(data processing \n",
      "& ML)\n",
      "Abstraction \n",
      "layer\n",
      "Hosting of Vector \n",
      "databases\n",
      "Web-interface \n",
      "Runtime \n",
      "environment\n",
      "(Kubernetes)\n",
      "Agent \n",
      "instances\n",
      "NNDIP\n",
      "Seldon\n",
      "(Model \n",
      "management)\n",
      "Other local platforms for \n",
      "analytics and AI projects \n",
      "(e.g., Dataiku)\n",
      "External \n",
      "Foundational \n",
      "models\n",
      "(OpenAI)\n",
      "Low-code\n",
      "(missing)\n",
      "(Local) dev \n",
      "environments\n",
      "Onboarding of unstructured \n",
      "data by GAIA team\n",
      "Abstraction \n",
      "layer\n",
      "The current technology stack varies based on use case requirements like RAG and ML\n",
      "Azure Data \n",
      "Factory\n",
      "Data layer\n",
      "Application \n",
      "layer\n",
      " PowerBI\n",
      "Collibra\n",
      " Unity Catalog\n",
      "Responsible AI (missing)\n",
      " Data quality (missing)Capabilities need for scaling data & AI\n",
      "8\n",
      "These building blocks must work together systematically (not piecemeal) to support \n",
      "scaled, safe AI use\n",
      "Governance\n",
      "(for both data and \n",
      "responsible AI)\n",
      "Marketplace \n",
      "for data product and \n",
      "AI-related assets\n",
      "Agentic AI \n",
      "capability\n",
      "Data engineering & \n",
      "reporting capability\n",
      "Data quality \n",
      "management\n",
      "Agentic AI is the focus of this presentation. However, the \n",
      "surrounding capabilities play a very important role for Agentic AI. \n",
      "Therefore, we also propose changes to other related capabilities.9\n",
      "What NNDIP + GAIA cover well:\n",
      "• Hosts both classical ML and GenAI models (LLMs)\n",
      "• Supports deployment of pre-processing transformers, post-processing logic, and external service \n",
      "integration (with help from the central team)\n",
      "• Provides functional monitoring, including drift detection, outlier detection, and latency tracking \n",
      "via Seldon\n",
      "• Enforces deployment conventions for governance, reuse, and lifecycle traceability\n",
      "• Allows onboarding of unstructured data (no reusable data lake yet)\n",
      "• Each pipeline is tailored to specific use cases\n",
      "What NNDIP + GAIA lack for scalable agentic AI:\n",
      "• No managed development environment or integrated AI playground\n",
      "• No baseline image with boilerplate code, scaffolded folder structures, scripts, and frameworks \n",
      "for various AI use cases\n",
      "• Lacks built-in support for persistent memory or context retention across requests\n",
      "• No paved road experience (e.g., quickly creating repositories, CI/CD pipelines, API endpoints, \n",
      "etc.)\n",
      "• Lacks mechanisms for agentic planning, complex orchestration, and communication buses (e.g., \n",
      "MCP servers, integration with API gateways, and event platforms)\n",
      "• Misses several AI governance (e.g., fallback handling), prompt governance (e.g., drift), cost \n",
      "governance (e.g., API chain costs), embedded security & access governance, ethical/compliance\n",
      "Heatmap of the maturity of NN’s Agentic AI capability\n",
      "Level 1: Obsolete / Missing\n",
      " Level 2: Developing Maturity\n",
      " Level 3: Acceptable Maturity\n",
      "Architecture of the agentic ecosystem and maturity assessment of current-state\n",
      "Base platform\n",
      "Knowledge management services\n",
      "Developer \n",
      "services\n",
      "Agentic engineering \n",
      "experience via \n",
      "Internal Developer \n",
      "Platform (IDP)\n",
      "AI playground and \n",
      "data self-service \n",
      "environment\n",
      "Managed AI \n",
      "developer \n",
      "platform\n",
      "Agentic services\n",
      "Interaction and integration services\n",
      "Managed runtime \n",
      "(Kubernetes)\n",
      "Base images \n",
      "(pre-selected frameworks, \n",
      "scaffolded folder structures)\n",
      "RAG (Vector and \n",
      "Embedding services)\n",
      "Short-term storage\n",
      "(cache)\n",
      "Retrieval services (web \n",
      "crawl)\n",
      "Document parse and \n",
      "embedding services\n",
      "Offering MCP services\n",
      "Access to Foundational \n",
      "Models\n",
      "Cognitive services\n",
      "Fine-tuning\n",
      "Integration services for APIs and events\n",
      "Experience-related services for voice, video, search\n",
      "Governance\n",
      "Observability\n",
      "Operations\n",
      "AI & ML model \n",
      "management\n",
      "AI asset \n",
      "marketplace\n",
      "Responsible AI \n",
      "Governance\n",
      "Responsible AI \n",
      "Governance\n",
      "Marketplace \n",
      "for data and \n",
      "AI-related \n",
      "assets\n",
      "Agentic AI \n",
      "capability\n",
      "Data analytics \n",
      "& reporting\n",
      "Data quality \n",
      "management\n",
      "Certain level of automation and self-serviceBuilding and scaling our own technology stack is challenging\n",
      "• Agentic AI Frameworks: The field of generative AI is advancing quickly, with new \n",
      "frameworks, such as Langchain, LangGraph, Pydantic AI, Llamalndex, CrewAI, \n",
      "AutoGen, and Semantic Kernel. Committing to a particular set of frameworks may \n",
      "limit your organization's ability to adapt to these advancements.\n",
      "• LLM of the day: New LLMs and LLM updates are released frequently, with today's \n",
      "releases surpassing the champions of yesterday. So, we need to be flexible in a way \n",
      "that LLMs can be changed.\n",
      "• Agent Protocols: Agent-to-agent and -application protocols like MCP , A2A, and ANP \n",
      "have emerged, with more on the way.\n",
      "• Diverse needs: Projects have different requirements, such as the need for low-code \n",
      "or no-code solutions for quick delivery or usage, while others may need pro-code \n",
      "approaches for complex application integration scenarios. Balancing these options \n",
      "enhances agility and responsiveness.\n",
      "10\n",
      "To enhance the organization’s effectiveness \n",
      "in leveraging generative AI, it is \n",
      "recommended to establish a Centralized \n",
      "Oversight Team. This team would be \n",
      "responsible for overseeing the latest \n",
      "developments, frameworks, tools, and best \n",
      "practices across various use cases. By closely \n",
      "monitoring advancements in the field, \n",
      "evaluating and selecting suitable \n",
      "frameworks, and setting standardized best \n",
      "practices, the team can quickly adapt the \n",
      "direction.\n",
      "The market is evolving with new AI products and features at an unprecedented paceSeveral AI patterns – there is no one size fits all\n",
      "11\n",
      "Query LLM\n",
      "(Generate)\n",
      "LLM\n",
      "(Evaluate)\n",
      "OutputOutput\n",
      "Web\n",
      "App Response\n",
      "•Invoking LLMs and APIs\n",
      "•No external data usage\n",
      "Query Embed\n",
      "prompt\n",
      "LLM\n",
      "queryApp\n",
      "•Invoking LLMs and APIs\n",
      "•Requiring a vector database\n",
      "•Executing Python scripts\n",
      "Vectors\n",
      "output\n",
      "input\n",
      "Response\n",
      "2. Retrieval-Augmented Generation (RAG) pattern\n",
      "1. Standalone agent pattern\n",
      "Query\n",
      "MCP \n",
      "Server\n",
      "App\n",
      "•Requiring hosting of MCP Servers\n",
      "•Interacting with external services\n",
      "Vectors\n",
      "MCP \n",
      "Server SaaS vendorAI application\n",
      "(Agent)\n",
      "MCP \n",
      "Client\n",
      "LLM\n",
      "Response\n",
      "Response\n",
      "Response\n",
      "Query\n",
      "App\n",
      "LLM\n",
      "Response\n",
      "Response\n",
      "query\n",
      "Vectors\n",
      "output\n",
      "State\n",
      "AI application\n",
      "(Agent)\n",
      "AI application\n",
      "(Agent)\n",
      "AI application\n",
      "(Agent)\n",
      "•Running multiple agents \n",
      "•Querying vector databases\n",
      "•Orchestration and usage of a store state\n",
      "4. Multi-agent pattern\n",
      "3. Model Context Protocol (MCP) pattern\n",
      "We need to have a flexible architecture for facilitating different scenarios (this is not an exhaustive overview)\n",
      "Base images* Managed runtime* Management*\n",
      "Retrieval Document parse Embedding\n",
      "Orchestration\n",
      "MCP serversComplex application integration\n",
      "Complex application integration\n",
      "Retrieval Document parse Embedding\n",
      "* Required for all scenariosRequirements\n",
      "• Fast delivery of the new architecture to meet the demands of our organization (several teams already want to start in Q3 2025).\n",
      "• Need for standardizing our IT foundations for tackling complexity and increased flexibility.\n",
      "• Recognize the need for different architecture runways:\n",
      "• Data engineering and/or machine learning for data-intensive use cases.\n",
      "• AI or automation-related development in the context of improving operational processes.\n",
      "• AI-related development aimed at providing rich context using large amounts of data.\n",
      "• Managing governance to adhere to external regulations and internal standards.\n",
      "• Recognize the need for different types of agents, such as operational agents that are context-aware and can operate with low latency in operational systems, as well as \n",
      "analytical agents that can serve processed datasets.\n",
      "• This translates to the necessity of working with different runtime environments (e.g., Databricks and Kubernetes) and deploying across various cloud environments.\n",
      "• A unified approach to structured and unstructured data management is essential, with data products that can be consumed quickly in other environments and the ability to \n",
      "seamlessly use data across different platforms for machine learning and AI.\n",
      "• We must remain flexible for the future by adopting open standards such as Delta, Iceberg, Spark, Kafka, and Flink, as well as open specifications like the Open Data Contracts \n",
      "Standard and the Open Data Product Specification.\n",
      "• Real-time data processing is critical\n",
      "• There is a strong need for a single and simplified governance process for onboarding AI and hyper-automation use cases, registration of data products, registration of AI-related \n",
      "artifacts, and more. This governance framework also encompasses agents that are integrated into SaaS solutions. It is important to enable every employee to generate value \n",
      "with AI, which necessitates the establishment of low-code standards. \n",
      "• There is also a need for high-quality data, which requires a robust data quality capability.\n",
      "• There is a strong desire for two different types of marketplaces: 1) one for collaborative co-creation of AI-related assets, and 2) one for certified data products, data quality, and \n",
      "related services.\n",
      "• Ensure interoperability between Business Units (BUs) for rapid data sharing and implement enterprise-wide data standardization to minimize redundant tasks for users.\n",
      "12\n",
      "High-level requirements that frame the future-state architectureComparing different types of Agents: application-oriented and data-oriented\n",
      "Application-oriented agents\n",
      "AI agents designed to function within real-time operational environments, focusing on \n",
      "immediate tasks and interactions.\n",
      "• Context-Aware: Capable of understanding and adapting to the current context in which \n",
      "they operate.\n",
      "• Low Latency: Optimized for quick responses and actions, crucial for operational efficiency.\n",
      "• System Interaction: Engage with various systems through APIs to retrieve or manipulate \n",
      "data as needed. Or require extra application components like a data-intensive application or \n",
      "Operational Data Store (ODS).\n",
      "• Real-Time Data Access: Able to fetch the most recent data to ensure decisions are based on \n",
      "the latest information.\n",
      "• Long-Running Processes: May be involved in complex, ongoing workflows that require \n",
      "orchestration across multiple systems.\n",
      "• Example use cases: Hyper-automation, real-time fraud detection agents, dynamic risk and \n",
      "pricing agents, contextual recommendation agents, real-time claims processing agents, and \n",
      "so on.\n",
      "13\n",
      "The importance of recognizing agent types\n",
      "Data-oriented agents\n",
      "AI agents focused on data processing, enriching, and analyzing data to derive insights and \n",
      "support decision-making.\n",
      "• Insight Generation: Provide analytical outputs that help in understanding patterns, \n",
      "forecasting, and making informed decisions.\n",
      "• Data Processing: Typically handle larger data sets in a batch manner rather than in \n",
      "real-time, allowing for comprehensive analysis.\n",
      "• Limited strong consistency requirements: Unlike operational agents, they may not \n",
      "require most accurate data for real-time data or transactional processing. Instead, \n",
      "these type of agents more typically for on deeper analytical tasks. So, latency is \n",
      "accepted to be higher.\n",
      "• Example use cases: Chatting with your own data, data enrichment agents, serving a \n",
      "historical profile, customer 360, big data processing agents, market research analysis \n",
      "agents, finance controlling agents, policy renewal prediction agents, regulatory \n",
      "compliance monitoring agents, etc.\n",
      "ML + Data-oriented agents\n",
      "Application-oriented agents\n",
      "Proposed future-state reference architecture for data & AI – scenario #1\n",
      "14\n",
      "Two run-time stacks for AI Agents: Kubernetes (K8s) and Databricks\n",
      "Integration layer\n",
      "API Gateway (WSO2)\n",
      " Event Streaming (Kafka + Flink)\n",
      "Operational Data Stores (PostgreSQL, Azure SQL, Lakebase)\n",
      "Governance services\n",
      "Data Intelligence Platform (Databricks)\n",
      "Databricks \n",
      "Apps\n",
      "Foundation \n",
      "model APIs\n",
      "Mlflow\n",
      "(model governance)\n",
      "AI \n",
      "Gateway\n",
      "AutoML\n",
      "AI \n",
      "Playground\n",
      "Mosaic AI Agent Framework\n",
      "(tooling for development, build and deploy)\n",
      "Model \n",
      "Serving\n",
      "Vector \n",
      "Search\n",
      "Agent Bricks\n",
      "Foundational Models, such \n",
      "as OpenAI & Anthropic\n",
      "Source system\n",
      "Data layer\n",
      "Application \n",
      "layer\n",
      "Inference runtime \n",
      "environment\n",
      "(Kubernetes)\n",
      "Agents & model \n",
      "instances\n",
      "Monitoring & \n",
      "managementZelda IDP\n",
      "Lightweight Python \n",
      "dev environment for \n",
      "developing \n",
      "operational-aligned \n",
      "agents\n",
      "Base images with \n",
      "Python libraries\n",
      "AI agent \n",
      "development, \n",
      "testing, deployment\n",
      "Tracking \n",
      "and tracing\n",
      "Data Analytics platform (Databricks)\n",
      "Lakehouse \n",
      "(Medallion design with Delta \n",
      "+ Iceberg for both structured \n",
      "and unstructured data)\n",
      "Spark (data \n",
      "processing & \n",
      "ML)\n",
      "Databricks \n",
      "SQL\n",
      "Databricks \n",
      "Dashboards\n",
      "PowerBI\n",
      "Databricks \n",
      "Lakeflow\n",
      "Low-code*\n",
      "Low-code\n",
      "* Recognize that various platforms (e.g., Salesforce, SAP, Adobe, Databricks, Microsoft 365 & Teams, Collibra) are increasingly incorporating pay-per-use \n",
      "low-code features tailored to their specific services. Overseeing this trend, we should utilize the embedded agents (configured via low-code features) \n",
      "offered by our chosen platforms to optimize development within specific contexts. \n",
      "Lakebase\n",
      "Collibra\n",
      " Unity Catalog\n",
      "Responsible AI (TBD)\n",
      "AI Collaborative Marketplace (TBD)\n",
      "Data Quality (TBD)\n",
      "Low-code\n",
      "Extra embedding, RAG, \n",
      "MCP services, etc.\n",
      "Low-codeProposed future-state reference architecture for data & AI – scenario #2\n",
      "15\n",
      "Databricks as a best-of-suite platform for all data and AI use cases (AWS and Azure)\n",
      "Source system\n",
      "Data layer\n",
      "Application \n",
      "layer\n",
      "Integration layer\n",
      "API Gateway\n",
      " Event Streaming (Kafka + Flink)\n",
      "Operational Data Stores\n",
      "Deploy and run on \n",
      "other environments, \n",
      "such as AWS\n",
      "Inference runtime \n",
      "environment\n",
      "(Kubernetes)\n",
      "Agent instances\n",
      "Inference outside DBX\n",
      "Data Analytics platform (Databricks)\n",
      "Lakehouse \n",
      "(Medallion design with Delta + \n",
      "Iceberg for both structured and \n",
      "unstructured data)\n",
      "Spark \n",
      "(data processing \n",
      "& ML)\n",
      "Databricks \n",
      "SQL\n",
      "Databricks \n",
      "Dashboards\n",
      "Databricks \n",
      "Lakeflow\n",
      "PowerBI\n",
      "Data Intelligence Platform (Databricks)\n",
      "Databricks \n",
      "Apps\n",
      "Foundation \n",
      "model APIs\n",
      "MLflow\n",
      "Model \n",
      "Serving\n",
      "AutoML\n",
      "AI \n",
      "Playground\n",
      "Mosaic AI Agent Framework\n",
      "AI \n",
      "Gateway\n",
      "Vector \n",
      "Search\n",
      "Agent Bricks\n",
      "(low-code)\n",
      "Foundational Models, such \n",
      "as OpenAI & Anthropic\n",
      "Lakebase\n",
      "Governance services\n",
      "Collibra\n",
      " Unity Catalog\n",
      "Responsible AI\n",
      "AI Marketplace (TBD)\n",
      "Data Quality (TBD)Proposed future-state reference architecture for data & AI – scenario #3\n",
      "16\n",
      "Best of breed approach (upscaling the existing architecture)\n",
      "Integration layer\n",
      "API Gateway\n",
      " Event Streaming (Kafka + Flink)\n",
      "Operational Data Stores\n",
      "Runtime environment\n",
      "(Kubernetes)\n",
      "Data-intensive \n",
      "applications\n",
      "Agent instances\n",
      "Zelda IDP\n",
      "Base images \n",
      "with  Python \n",
      "libraries\n",
      "AI development, \n",
      "testing, \n",
      "deployment\n",
      "Tracking and \n",
      "tracing\n",
      "NNDIP\n",
      "Seldon\n",
      "(Model \n",
      "management)\n",
      "Other local \n",
      "platforms for \n",
      "analytics and \n",
      "AI projects \n",
      "(e.g., Dataiku)\n",
      "GAIA\n",
      "Foundational \n",
      "Models, such as \n",
      "OpenAI & \n",
      "Anthropic\n",
      "Abstraction layer\n",
      "Hosting of Vector \n",
      "databases\n",
      "Web-interface \n",
      " Low-code\n",
      "(TBD)\n",
      "Source system\n",
      "Data layer\n",
      "Application \n",
      "layer\n",
      "NNDAP\n",
      "Lakehouse \n",
      "(Medallion design with Delta + \n",
      "Iceberg for both structured \n",
      "and unstructured data)\n",
      "Spark (data \n",
      "processing & \n",
      "ML)\n",
      "Databricks \n",
      "SQL\n",
      "Databricks \n",
      "Dashboards\n",
      "PowerBI\n",
      "Databricks \n",
      "Lakeflow\n",
      "Lakebase\n",
      "Governance services\n",
      "Collibra\n",
      " Unity Catalog\n",
      "Responsible AI\n",
      "AI Marketplace (TBD)\n",
      "Data Quality (TBD)\n",
      "Serving context and \n",
      "LLM interactionConsiderations for all three scenarios\n",
      "Pros:\n",
      "• Reduces infrastructure complexity and associated costs by using a \n",
      "single platform.\n",
      "• Faster time to market: A best-of-suite approach accelerates time to \n",
      "market by providing integrated, streamlined solutions that simplify \n",
      "deployment, reduce complexity, and enhance user experience.\n",
      "• Simplified governance and guidance to all teams.\n",
      "• High level of trust. Communities and guilds are already actively and \n",
      "enthusiastically engaging with Databricks.  \n",
      "Cons:\n",
      "• Databricks also must be hosted on AWS.\n",
      "• NNDIP and GAIA must be re-designed/re-platformed.\n",
      "• Databricks will be used for all types of workloads, including \n",
      "lightweight tasks that may not require its full capabilities. This could \n",
      "lead to suboptimal utilization of resources and features .\n",
      "• May not accommodate specialized edge cases that require \n",
      "alternative deployment patterns. For instance, high-latency use cases \n",
      "may not perform optimally if platform doesn't meet specific needs.\n",
      "• Heavy reliance on a single provider, which can lead to challenges if \n",
      "switching platforms becomes necessary.\n",
      "17\n",
      "#2 Best-of-suite approach with Databricks \n",
      "#1 Two run-time stacks for AI Agents: \n",
      "Kubernetes (K8s) and Databricks\n",
      "In this scenario, Azure Databricks serves as the comprehensive \n",
      "platform for all data and AI use cases. It simplifies development by \n",
      "providing integrated tools for data processing, machine learning, and \n",
      "collaboration. This approach is particularly cost-effective and well-\n",
      "suited for high-latency applications running within the Azure \n",
      "ecosystem. However, it may fall short for certain edge cases that \n",
      "necessitate deploying models and agents on Kubernetes in AWS.\n",
      "A dual approach might be more desired for more a specialized \n",
      "development processes. This scenario employs a hybrid architecture \n",
      "where Databricks (both AWS and Azure) is used for most data \n",
      "integration and AI development, while a separate tech stack utilizing \n",
      "Python and Agentic frameworks is deployed for specific use cases. The \n",
      "Python stack runs on containerized environments, potentially on \n",
      "Kubernetes, and integrates back to Azure Databricks for experiment \n",
      "and model tracking and tracing via managed MLflow in Databricks. This \n",
      "provides flexibility for edge cases while retaining a strong central \n",
      "registry in Databricks.\n",
      "This scenario could be implemented organically, next to scenario #2, \n",
      "depending on results and specific use cases.\n",
      "Pros:\n",
      "• Allows for the development of tailored solutions using Python and \n",
      "other frameworks, which may be more suitable for certain tasks.\n",
      "• Spreads the risks by not relying solely on a single vendor, allowing for \n",
      "adaptability in case market dynamics change and ensuring more \n",
      "resilience against vendor-related issues.\n",
      "Cons:\n",
      "• More systems to manage can complicate the architecture and require \n",
      "more robust governance and oversight.\n",
      "• Extra integration effort: Ensuring seamless integration between \n",
      "Databricks and the Python tech stack could require additional effort \n",
      "and resources.\n",
      "• Teams may require extra needs in expertise in multiple technologies, \n",
      "increasing the training and onboarding burden.\n",
      "Weighting the pros and cons of the different implementation scenarios\n",
      "#3 Best-of-breed approach\n",
      " (upscaling the existing architecture)\n",
      "In this best-of-breed architecture, we will leverage the strengths of \n",
      "various services and frameworks to build a modular, flexible, and \n",
      "scalable data integration and AI architecture. The architecture will \n",
      "utilize Azure Databricks for data engineering, alongside a containerized \n",
      "Python stack for specialized workloads, enabling seamless integration \n",
      "and optimal resource utilization.\n",
      "Pros:\n",
      "• Facilitates the creation of customized solutions optimizing \n",
      "performance for specific use cases.\n",
      "• More effectively reuse of what we currently already have.\n",
      "• Reduces dependency on a single vendor \n",
      "• Increased adaptability to market changes \n",
      "Cons:\n",
      "• Requires a long time to market due to the complexity of managing a \n",
      "hybrid architecture and ensuring all components work together \n",
      "effectively. This is not desired. \n",
      "• This scenario is challenging given the team's lack of experience and \n",
      "the need to cultivate a strong engineering culture to support such an \n",
      "architecture.\n",
      "PreferenceConsequences and implications for the existing architecture\n",
      "• Implement a dual-platform strategy for AI development: Use Databricks for data-oriented AI and Zelda IDP for operational AI. Establish strong governance for use case allocation. Develop standardized \n",
      "workflows and shared libraries.\n",
      "• Enable Databricks as the global data and AI platform: Deployable on both Azure and AWS. Build and deploy AI agents on Databricks, including in Kubernetes.\n",
      "• Create a unified control plane for AI and data services across Azure and AWS: Support provisioning for various use cases. Simplify provisioning and management.\n",
      "• Promote Databricks Lakeflow in favor over Azure Data Factory: Transition period required due to Lakeflow's limited source compatibility.\n",
      "• Enable serverless capabilities for Databricks services: IP address utilization review and possibly restructuring the current VNET architecture to remove implications. \n",
      "• Introduce Databricks Dashboards next to Power BI: Facilitate native analytics workflows with Databricks Dashboards, Databricks Designer and Databricks One. Decrease time-to-insight for users.\n",
      "• Standardize ML lifecycle management using MLflow on Databricks: Shift away from Seldon. Conduct impact assessments for model performance features.\n",
      "• Transition GAIA to Databricks: Utilize Databricks Vector Search and Databricks Apps for future RAG applications. Stop onboarding new GAIA use cases once new architecture is ready.\n",
      "• Implement dual marketplaces for AI and data products: Use Collibra as the data product marketplace. Define a collaborative AI marketplace with engineering community input.\n",
      "• Establish enterprise data quality capability: Select a robust data quality tool that integrates with existing platforms. Automate checks for data accuracy and completeness.\n",
      "• Improve responsible AI governance for agents and hyper-automation: Introduce streamlined onboarding processes. Include responsible AI tooling for compliance and monitoring.\n",
      "• Commit to open standards and specifications: Favor open data and AI standards to avoid vendor lock-in. Ensure standards are reflected in design decisions.\n",
      "• Unify management of structured and unstructured data: Support hybrid data types and cross-environment accessibility.\n",
      "• Allow low-code features within bounded contexts: Utilize embedded agents configured via low-code capabilities offered by various platforms (Salesforce, Microsoft 365, SAP, Adobe, and so on) to \n",
      "optimize development within specific contexts instead of relying on a single low-code platform.\n",
      "• Establish governance and guidelines for standardized frameworks and LLMs: Create comprehensive governance frameworks that define standards for using allowed large language models and other AI \n",
      "technologies. Develop guidelines to ensure compliance, quality, and considerations across all AI initiatives.\n",
      "• No AI and Serverless features expansions to existing localized DAP Environments: Teams, such as SIDAP and LPDAP, must plan and execute migration strategies to the new platformized future-state \n",
      "NNDAP & NNDIP architecture for leveraging new features. \n",
      "• Certain products and services — such as DataLab and RAG — will be delivered as managed services. This requires a clear split of responsibilities between EBT and EET. EBT will be responsible for \n",
      "designing, delivering, and maintaining these functional services. EET will offer the infrastructure building blocks and operational support layer.\n",
      "• Enable Python as a general-purpose foundation across all data and AI activities: This requires a Python ecosystem, including a package and dependency manager (UV), an IDE, a curated set of libraries \n",
      "(such as FastAPI, FastMCP, Seaborn, Pandas, Dart, Streamlit, PySpark, PyFlink, etc.), integration into Zelda IDP, observability (Logfire), and more.\n",
      "18\n",
      "Proposed changesData Intensive Applications Platform\n",
      "19\n",
      "Integration layer\n",
      "Orchestration\n",
      "(Camunda)\n",
      "Data & AI Governance Layer\n",
      "Data Management solution\n",
      "(Collibra)\n",
      "Asset Management Solution\n",
      "(Unity Catalog)\n",
      "Data Analytics PlatformData Intelligence Platform \n",
      "Responsible AI solution\n",
      "(RfP)\n",
      "Marketplaces\n",
      "(Collibra, Databricks, IDP)\n",
      "Event & Data Streaming\n",
      "(Confluent Kafka)\n",
      "API Management\n",
      "(WSO2  & Azure API Gateway)\n",
      "DQ solution\n",
      "(RfP)\n",
      "Transactional Systems\n",
      "Systems of Record\n",
      "(SAP ERP, SAP FS-PM, Oxygen, TIA, Fineos, etc) \n",
      "Dashboarding \n",
      "& Reporting\n",
      "AI Engineering\n",
      "Systems of \n",
      "Intelligence\n",
      "Systems of \n",
      "Record\n",
      "Systems of Engagement\n",
      "(WebPortals, Mobile Apps, DXP, etc) \n",
      "Stream Processing\n",
      "(Apache Flink)\n",
      "Data Ingestion Data Platform\n",
      "Data Storage\n",
      "(DBX Lakehouse)\n",
      "Data Processing\n",
      "(Spark)\n",
      "Databricks Lakeflow\n",
      "Databricks \n",
      "Autoloader\n",
      "Databricks \n",
      "Dashboards\n",
      "PowerBI\n",
      "Databricks SQL\n",
      "Azure Data Factory\n",
      "ObservabilityCompute \n",
      "Serving Data Serving\n",
      "Model Runtime\n",
      "(Portable Stack)\n",
      "Application \n",
      "Observability\n",
      "(OTel)\n",
      "Application Runtime\n",
      "(Potable Stack)\n",
      "Feature Store\n",
      "(tbd)\n",
      "ODS \n",
      "(tbd)\n",
      "Agent Observability\n",
      "(MLFlow)\n",
      "AI Agent Runtime\n",
      "(Portable Stack)\n",
      "Model\n",
      "Observability\n",
      "(MLFlow)\n",
      "ObservabilityCompute \n",
      "Serving Data Serving\n",
      "AI Agent Runtime\n",
      "(Mosaic)\n",
      "AI Vector DB\n",
      "(Mosaic)\n",
      "AI Agent \n",
      "Observability\n",
      "(MLFlow)\n",
      "AI Gateway\n",
      "(Mosaic)\n",
      "AI Agent Evaluation\n",
      "(Mosaic)\n",
      "AI Gateway\n",
      "(MLFlow)\n",
      "Stream Processing\n",
      "(Spark)\n",
      "Lakehouse \n",
      "Monitoring\n",
      "Agent Integration\n",
      "(MCP, A2A)\n",
      "Systems of \n",
      "Integration\n",
      "Data EngineeringData Intensive Application & AI Engineering\n",
      "Agentic Framework\n",
      "(Mosaic)\n",
      "Agentic Framework  \n",
      "(LangChain)\n",
      "Low Code\n",
      "Agent Bricks\n",
      "Agentic Framework  \n",
      "(LangGraph)\n",
      "High CodeLow CodeHigh Code\n",
      "IDP\n",
      "(Zelda)\n",
      "Agentic Framework\n",
      "(Pydantic)\n",
      "Low Code\n",
      "(Dataiku)\n",
      "Low CodeHigh Code\n",
      "Vaultspeed Studio\n",
      "DDP\n",
      "(Zelda)\n",
      "Target operating model\n",
      "2020\n",
      "Data & AI Control Tower (Checks & Balances)\n",
      "Data & AI Strategy Data & AI Policies & Standards Data & AI Governance Data & AI  Portfolio Mngt\n",
      "Cloud Landing Zone’s\n",
      "Productized & Platformized Services\n",
      "Public Cloud AWS Public Cloud Azure Private Cloud\n",
      "Managed IaaS Platform Portable Platform Cloud Native Platforms Data & AI  Platforms\n",
      "- Productized Databricks- Productized K8s, Productized Camunda\n",
      "- Productized Kafka\n",
      "- Productized Services AWS\n",
      "- Productized Services Azure\n",
      "NNDAP – Data Analytics Platform NNDIP – Data Intelligence Platform\n",
      "Databricks \n",
      "Lakehouse\n",
      "Databricks SQL Databricks BI\n",
      "Databricks Lakeflow Databricks \n",
      "Mosaic AI\n",
      "MLOps/AIOps \n",
      "(Seldon and Databricks)\n",
      "RAG \n",
      "(GAIA and Databricks)\n",
      "Bricks Agents\n",
      "NNDAG – Data & AI Governance Platform\n",
      "Databricks Unity \n",
      "Catalog\n",
      "AI Governance (tbd) Data Quality (tbd)\n",
      "Collibra\n",
      "Group IT TBD\n",
      "EET Enterprise Engineering Technology ENABLES the business by delivering the tech & platforms\n",
      "EBT Enterprise Business Technology EMPOWERS the business by delivering knowledge, guidelines, best practices\n",
      "DAI Data & AI Control Tower ENSURES we are doing the right things right21What we propose\n",
      "• Our ambition is to kick-start the development of AI use cases in the third quarter of 2025. Therefore, we propose to identify a small set \n",
      "of diverse use cases (bodily injury, marketing content advisor, and helpdesk assistant) to test the best-of-suite approach and gradually \n",
      "develop G-IT services.\n",
      "• For our collaboration with Databricks, we recommend implementing a more structured and controlled collaboration model. We find \n",
      "this necessary to ensure alignment with our organizational goals and to mitigate potential risks. Databricks is actively engaging with \n",
      "various teams within the organization and promoting the rapid adoption of new features. However, the introduction of these new \n",
      "features should be approached with caution, as certain features may incur high costs and/or present significant lock-in risks.\n",
      "• AI is evolving at an incredible speed, and we cannot predict who will emerge as the leader of tomorrow. Therefore, we also \n",
      "recommend investing in a secondary developer platform that can also address the concerns we have regarding use cases requiring \n",
      "tighter integration with operational systems.\n",
      "• We advocate for horizontal governance across the different tech stacks to ensure that best practices are followed and frameworks are \n",
      "standardized. Data governance and the associated workflows must be seamlessly integrated into the overall strategic plan.\n",
      "22Appendix\n",
      "23Progressively improving the maturity\n",
      "24\n",
      "Bodily injury\n",
      "(Non-life)\n",
      "Legacy x\n",
      "(International) ….\n",
      "Governance\n",
      "services L1/L2 L2 L2/3 L3\n",
      "Developer \n",
      "services L1 L1/L2 L2 L3\n",
      "Interaction & \n",
      "integration L2 L2 L2 L3\n",
      "Knowledge \n",
      "management L2 L2 L2 L3\n",
      "Agentic services L2 L2 L2 L3\n",
      "Base platform L1 L2 L2/3 L3 L3 | Acceptable maturity\n",
      "L2 | Developing maturity\n",
      "L1 | Obsolete / Missing\n",
      "As-is Plateau 1 Plateau 2 Plateau X\n",
      "Q3’25\n",
      "Governance services: We propose \n",
      "prioritizing the strengthening of data \n",
      "governance as it is essential for ensuring \n",
      "compliance with regulatory requirements, \n",
      "enhancing data quality, and building trust \n",
      "in our AI systems.\n",
      "Scalable platform: We propose making the \n",
      "enhancement of our platform services a \n",
      "top priority to facilitate seamless \n",
      "integration and scalability of AI capabilities \n",
      "across various business units. \n",
      "Enhancing the maturity for the AI-related services at every plateauSystematic approach for delivering AI use cases\n",
      "Showing the architectural needs for the end-to-end process\n",
      "Data engineering & API \n",
      "management\n",
      "(if not available)\n",
      "Develop data pipeline, \n",
      "design datasets\n",
      "Design API endpoint\n",
      "Identify and register \n",
      "application\n",
      "Data engineering\n",
      "Hand-over data script(s) to \n",
      "data engineering team\n",
      "Develop data pipeline, \n",
      "design datasets\n",
      "Register new \n",
      "data products\n",
      "Operationalize\n",
      "Register final artifacts\n",
      "Optimize scripts (remove \n",
      "hard coded values, etc.)\n",
      "Replace feature \n",
      "engineering code with \n",
      "input dataset\n",
      "Register parameters\n",
      "Implement CI/CD process \n",
      "(Go-live)\n",
      "Monitor\n",
      "Technical logging and \n",
      "track behavior\n",
      "Evaluate, bias detection, \n",
      "model drift, ECF controls\n",
      "Conduct A/B or Multi-\n",
      "Armed Bandit Testing\n",
      "Initiate project\n",
      "Define business \n",
      "opportunity\n",
      "Create new DevOps \n",
      "project\n",
      "Initialize or clone git \n",
      "repository\n",
      "Experiment #1\n",
      "Define agentic requirements \n",
      "(runtime + libraries)\n",
      "Develop Python scripts\n",
      "Request data products\n",
      "Feature engineering / \n",
      "Chunking data\n",
      "Track experiments\n",
      "Register and version \n",
      "artifacts (datasets, scripts, \n",
      "models)\n",
      "Snapshot code or tag code\n",
      "Log metrics\n",
      "Experiment #n\n",
      "Iterate\n",
      "Exploratory Phase Operational Phase\n",
      "End project\n",
      "Update responsible AI \n",
      "assessment\n",
      "Initial responsible AI \n",
      "assessment\n",
      "Assign data ownership \n",
      "at source\n",
      "Registration/ \n",
      "Decommission\n",
      "Update responsible AI use \n",
      "case end report\n",
      "Update model catalog\n",
      "Update data catalog\n",
      "Go-live & share \n",
      "knowledge\n",
      "Perform cross-team \n",
      "learning  \n",
      "… NNDIP\n",
      "… Collibra\n",
      "… Azure DevOps, GitHub\n",
      "… Responsible AI tooling\n",
      "… NNDAP\n",
      "Perform cross-team \n",
      "learning  \n",
      "Secondary responsible AI \n",
      "assessment\n",
      "Search marketplace for \n",
      "assets / artifacts\n",
      "Register (data) products \n",
      "Cleanup and archive \n",
      "resources\n",
      "Security assessment26\n",
      "Unstructured \n",
      "documents, \n",
      "files, etc.\n",
      "Real-time\n",
      "data\n",
      "Operational\n",
      "systems\n",
      "Vector \n",
      "Search \n",
      "Index\n",
      "Data Lake \n",
      "(Bronze)\n",
      "Embedding\n",
      "Data Lake \n",
      "(Silver)\n",
      "Document \n",
      "parsing \n",
      "frameworks \n",
      "& LLMs\n",
      "AI \n",
      "Gateway\n",
      "Conversational \n",
      "Experience\n",
      "Users\n",
      "External foundational \n",
      "models, such as OpenAI\n",
      "Databricks \n",
      "Apps\n",
      "Structured \n",
      "data\n",
      "(Bronze)\n",
      "Databricks Spark + \n",
      "Declarative Pipelines\n",
      "Structured \n",
      "data\n",
      "(Silver)\n",
      "Structured \n",
      "data\n",
      "(Gold)\n",
      "Databricks Spark + \n",
      "Declarative Pipelines\n",
      "RAG & Chatbot pattern (GAIA implementation on Databricks)27\n",
      "Operational\n",
      "system #2\n",
      "Model Serving\n",
      "Playground\n",
      "Models\n",
      "Evaluation Framework\n",
      "Tracing\n",
      " Review\n",
      "Unity Catalog\n",
      "Function\n",
      "Agent Framework\n",
      "External foundational \n",
      "models, such as OpenAI\n",
      "AI \n",
      "Gateway\n",
      "GenAI agent pattern (data-oriented agent with RAG on Databricks)\n",
      "Operational\n",
      "system #1\n",
      "Vector \n",
      "Search \n",
      "Index\n",
      "Input data \n",
      "for RAG\n",
      "Embedding\n",
      "Medallion \n",
      "Pattern\n",
      "28\n",
      "Self-service analytics environment on Databricks with Collibra\n",
      "Bronze \n",
      "data\n",
      "Databricks Spark + \n",
      "Declarative Pipelines\n",
      "Operational\n",
      "systems\n",
      "Silver \n",
      "data\n",
      "Gold\n",
      "data\n",
      "Databricks Spark + \n",
      "Declarative Pipelines\n",
      " Databricks Serverless\n",
      "Local \n",
      "storage\n",
      "Databricks Dashboards\n",
      "Databricks Designer\n",
      "Self-service environment\n",
      "Data Product \n",
      "Owner\n",
      "1. Publish data \n",
      "product workflow\n",
      "Collibra\n",
      "Data \n",
      "Consumer\n",
      "Publish data product\n",
      "3. Approve \n",
      "data usage \n",
      "workflow\n",
      "Create AD Group for \n",
      "certified data product\n",
      "Approve data usage\n",
      "Legal / compliance\n",
      "(extra approval?)\n",
      "Active \n",
      "Directoryautomatic identity management sync\n",
      "1b. sub-step\n",
      "(certified)\n",
      "Add AD user \n",
      "to AD groupGive AD Group access \n",
      "to Delta tables in UC\n",
      "1\n",
      "2\n",
      "3\n",
      "Sailpoint\n",
      "Manager \n",
      "approval\n",
      "2. Request data \n",
      "product workflow\n",
      "Request data products\n",
      "Data Office\n",
      "Certify data product\n",
      "Tables\n",
      "Unity Catalog\n",
      "Access controlLineage\n",
      "Process flow:\n",
      "1. Publish Data Products:\n",
      "• Data assets (Delta tables) are \n",
      "published in Unity Catalog.\n",
      "• Collibra scans the Unity \n",
      "Catalog.\n",
      "• A new data product is created \n",
      "by the data product owner, \n",
      "selecting the relevant data \n",
      "assets.\n",
      "• After publishing, a member of \n",
      "the data office team must \n",
      "review and approve the data \n",
      "product.\n",
      "• Once approved, the data \n",
      "product is certified, a new \n",
      "Active Directory (AD) group will \n",
      "be created. This group gains \n",
      "access to the Delta tables \n",
      "associated with the data \n",
      "product.\n",
      "2. Request Data Product:\n",
      "• Users can request data \n",
      "products, initiating a new \n",
      "request that must be reviewed.\n",
      "3. Approve Data Product:\n",
      "• The data product owner \n",
      "reviews and approves the \n",
      "request.\n",
      "• Optionally, approval may also \n",
      "be required from legal, \n",
      "compliance, or the data office.\n",
      "• Upon approval(s), the \n",
      "requested identity will be \n",
      "added to the AD group, using \n",
      "the Salespoint process.\n",
      "Operational\n",
      "system #1\n",
      "External foundational \n",
      "models, such as OpenAI\n",
      "GenAI agent pattern (low-latency operational-oriented agents)\n",
      "Operational\n",
      "system #2\n",
      "Operational\n",
      "system #3\n",
      "Developer \n",
      "Control\n",
      "Backstage (Zelda IDP)\n",
      "Azure DevOps AI paved road specification IaC\n",
      "IDE\n",
      "Internal package \n",
      "repositories with \n",
      "frameworks like Langchain, \n",
      "Pydantic, LangGraph, etc.\n",
      "(Nexus)\n",
      "Developer \n",
      "Control\n",
      "Container \n",
      "register\n",
      "DevOps \n",
      "action\n",
      "Kubernetes environment\n",
      "AgentAgentAgent\n",
      "PostgreSQL\n",
      "database\n",
      "Resources\n",
      "Web appALFRED (example of a finance business application on Databricks)\n",
      "30\n",
      "Real-time\n",
      "data\n",
      "Operational\n",
      "systems\n",
      "NEF Landing \n",
      "Zone\n",
      "(Lakehouse)\n",
      "Databricks Spark + \n",
      "Declarative Pipelines\n",
      "Enriched \n",
      "Zone\n",
      "(Lakehouse)\n",
      "Harmonized \n",
      "zone\n",
      "(Lakehouse)\n",
      "Databricks Spark + \n",
      "Declarative Pipelines\n",
      "Databricks \n",
      "Apps\n",
      "(web app)\n",
      "Landing \n",
      "area\n",
      "Agent\n",
      "(Validate and \n",
      "transform)Users \n",
      "uploading \n",
      "input files\n",
      "Databricks Spark + \n",
      "Declarative Pipelines\n",
      "Operational \n",
      "data mall\n",
      "(Lakebase)\n",
      "Collibra\n",
      "Lineage Glossary DQ Results\n",
      "Regulatory requirements\n",
      "RDM data\n",
      "QRT\n",
      "ECAPS\n",
      "ECAPS\n",
      "Agentic layer\n",
      "Vector \n",
      "Search \n",
      "Index\n",
      "Embedding\n",
      "Agent\n",
      "Agent\n",
      "Agent\n",
      "Agent\n",
      "Databricks \n",
      "Apps\n",
      "(web app)\n",
      " Users \n",
      "downloading \n",
      "data\n",
      "Databricks SQL\n",
      "DDA \n",
      "Agreements\n",
      "DDA \n",
      "Agreements\n",
      "NEF Interface \n",
      "standards\n",
      "LDM\n",
      "Metadata management\n",
      "NEF Mapper\n",
      "RDM Tool\n",
      "Data products\n",
      "Markets \n",
      "data\n",
      "MEDM / MIP\n",
      "NEF \n",
      "Validation\n",
      "Calculation \n",
      "engines\n",
      "Regulatory \n",
      "chatbot\n",
      "regulatory \n",
      "compliance \n",
      "monitoring \n",
      "agents\n",
      "Finance \n",
      "controlling \n",
      "agent\n",
      "NEF Mapping \n",
      "validation agentWhat International proposes\n",
      "31\n",
      "Transitioning from a best-of-breed approach to a best-of-suite approach\n",
      "The aim is to utilize more managed services and eliminate external \n",
      "resources, facilitating easier integration\n",
      "1\n",
      "Using fewer components simplifies the engineering process and reduces \n",
      "the likelihood of bugs\n",
      "2\n",
      "The local DAP initially began using Databricks as a processing engine, \n",
      "however, with the availability of more out-of-the-box components, we are \n",
      "transitioning to a complete Lakehouse implementation on Databricks\n",
      "3\n",
      "Databricks is a leader in Lakehouse architecture and continues to \n",
      "introduce new features\n",
      "4\n",
      "Azure\n",
      "Azure Services Databricks\n",
      "Custom \n",
      "developed \n",
      "components\n",
      "Azure\n",
      "Azure \n",
      "Services Databricks\n",
      "Custom \n",
      "developed \n",
      "components\n",
      "BEST OF \n",
      "BREED\n",
      "BEST OF \n",
      "SUITE32\n",
      "Sources Use CaseTransform and ProcessExtract / Ingest\n",
      "Structured data\n",
      "Semi-structured data\n",
      "Unstructured data\n",
      "Output\n",
      "Data\n",
      "Data Processing\n",
      "Automation\n",
      "Delta Live Tables\n",
      "On-demand/Analyses processing\n",
      "Batch\n",
      "Data Intelligence Engine\n",
      "SRC\n",
      "SRC\n",
      "CRM\n",
      "JSON\n",
      "Streaming\n",
      "Data Management\n",
      "Serverless\n",
      "Lakehouse federation\n",
      "Job Clusters Workflow\n",
      "Predictive IO\n",
      "Azure DevOps DataOps\n",
      "All-purpose \n",
      "Cluster\n",
      "Serverless SQL \n",
      "warehouse \n",
      "Predictive optimization\n",
      "Unity Catalog\n",
      "ODS\n",
      "Collibra\n",
      "Data Architecture\n",
      "Talend\n",
      "SilverBronze Gold\n",
      "Data Platform Governance\n",
      "Row/Column level security Catalog & Lineage\n",
      "Volumes Delta Tables\n",
      "Data Exchange\n",
      "Data Platform\n",
      "Structured data\n",
      "SRC\n",
      "SRC\n",
      "CRM\n",
      "Delta Sharing\n",
      "Assistant\n",
      "Access Control\n",
      "MLOps\n",
      "Streaming\n",
      "Event Hub\n",
      "Spark Streams\n",
      "CDC\n",
      "Databricks \n",
      "connectors\n",
      "Storage ADLS Gen 2\n",
      " Azure SQL DB\n",
      "Data Quality in Databricks\n",
      "API\n",
      "APIM\n",
      "LLMops\n",
      "International Data Platform Capabilities – Target Reference Implementation\n",
      "Modern Data Lakehouse build on top of Azure Databricks should implement the following services\n",
      "* Databricks Lakeflow Connectors show considerable potential, though some connectors remain in the preview stage and have yet to be fully tested.\n",
      "Fivetran connectors in \n",
      "Databricks\n",
      "JDBC connectors\n",
      "Data Science & Machine Learning\n",
      "Mosaic AI AutoMLMLflow\n",
      "Self-service & Sandboxing\n",
      "LAB environmentCloud sandboxPower Apps\n",
      "Data Analysis & Investigation\n",
      "MS Fabric or Databricks \n",
      "Notebooks & Dashboards\n",
      "AI/BI Genie space\n",
      "Reporting\n",
      "Databricks Dashboards\n",
      "MS Fabric\n",
      "Centralized Use Cases\n",
      "NNI DIP\n",
      "NNDIP (Seldon)\n",
      "Group reporting\n",
      "NN  GAIA\n",
      "Autoloader\n",
      "33\n",
      "Key concepts\n",
      "• Mosaic AI converts your Databricks platform into a comprehensive AI agent factory, allowing you to design and \n",
      "deploy AI agents using your existing data infrastructure without the need for additional tools. In contrast, Azure AI \n",
      "Foundry is more complex to manage, less integrated, and requires additional Azure services.\n",
      "• Unity Catalog functions as the central security feature, facilitating detailed governance across both AI components \n",
      "and data resources. It offers a unified management system that accommodates different permission rules for each \n",
      "resource type. It is important to overwatch which agent has access to which data. \n",
      "• AI agents utilize LLM engines to effectively handle natural language, engage in reasoning, and produce responses \n",
      "akin to human interaction. The MLflow AI Gateway facilitates seamless integration with foundation models from \n",
      "various providers, including OpenAI, Anthropic, Google, as well as offering the option to employ open-source \n",
      "models. Remaining LLM agnostic is a key feature that enables swift adaptation in this rapidly changing field.\n",
      "• These agents integrate with your business systems via specialized tools (such as database connectors, API clients, \n",
      "and semantic search), while upholding strict enterprise security standards enforced by Unity Catalog. To simplify \n",
      "access to business system, Model Context Protocol (MCP) establishes standardized, secure connections between \n",
      "your AI agents and enterprise data sources, reducing integration complexity while maintaining stringent security \n",
      "measures. \n",
      "• Additionally, Databricks enables real-time ingestion of event streams (such as Kafka or Event Hubs) directly into \n",
      "Delta lake with sub-second latency, ensuring AI agents have access to recent data for downstream applications.\n",
      "• Mosaic AI is Agentic AI agnostic, all the major frameworks such as Langchain, LangGraph, LlamaIndex, PydenticAI, \n",
      "CrewAI are supported by Databricks.\n",
      "• Once operational, Mosaic AI Agent Evaluation continuously assesses quality, cost, and performance using \n",
      "specialized LLM evaluators, offering metrics and insights to enhance agents' performance over time.\n",
      "• Maintain a global component library in Databricks Unity Catalog for standardized resources.\n",
      "Risks\n",
      "• Technology Maturity: The Mosaic AI Gateway and Agent Framework were launched in mid-2024. As they are still in \n",
      "the Public Preview stage, along with the support for leading frameworks LangChain and LangGraph, users should be \n",
      "aware of potential ongoing development and adjustments.\n",
      "• Agentic Protocols (MCP, A2A): While these protocols are currently being introduced, they have yet to become \n",
      "industry standards. This situation could lead to significant changes in the future.\n",
      "Downstream \n",
      "Application \n",
      "Integration\n",
      "Mosaic AI Gateway\n",
      "Unity Catalog\n",
      "APIs\n",
      "Model Context \n",
      "Protocol (MCP)\n",
      "Events\n",
      "NNI DAP (L1+L2)\n",
      "L3\n",
      "Data Products\n",
      "(ICDM, Feature \n",
      "Store)\n",
      "Data Lakehouse \n",
      "Layers \n",
      "(Bronze / Silver / \n",
      "Gold)\n",
      "L2\n",
      "Multitenant\n",
      "Serving Metadata\n",
      "• Human \n",
      "feedback\n",
      "• Metric tables\n",
      "• Inference \n",
      "tables\n",
      "Foundation \n",
      "Models\n",
      "Mosaic AI Vector \n",
      "Database\n",
      "Mosaic AI Agent \n",
      "Framework\n",
      "Mosaic AI \n",
      "Agent \n",
      "Evaluation\n",
      "Lakehouse\n",
      "Monitoring\n",
      "MLflow AI \n",
      "Gateway\n",
      "MLflow \n",
      "Tracking \n",
      "Server\n",
      "Agent to Agent \n",
      "Protocol (A2A)\n",
      "Database links\n",
      "NNI Mediation \n",
      "Layer\n",
      "Proposed International Agentic AI Architecture\n",
      "The agentic AI architecture primary uses Mosaic AI\n",
      "33\n",
      "NNI Agentic AI Platform\n",
      "Self-Service Front-End\n",
      "for Citizen deployment (NNI BUs)\n",
      "(Step 2)\n",
      "Protocol \n",
      "Abstraction LayerImplications for other Business Units\n",
      "34\n",
      "The scope of international mostly touches the analytical side of the architecture\n",
      "Conversational layer\n",
      "(interfaces for web, speech, search, instructions, etc.)\n",
      "Agentic layer\n",
      "(query resources, execute tasks, perform reflections, orchestrate, etc.)\n",
      "Context-aware layer\n",
      "APIs\n",
      "Operational systems\n",
      "Context-aware layer\n",
      "Analytical platforms\n",
      "(structured + unstructured)\n",
      "SQLGenAI\n",
      "ML Runtime\n",
      "ML\n",
      "GenAI Runtime\n",
      "Reports\n",
      "Scope International\n",
      "Implications for other business units\n",
      "• With international operations primarily utilizing Azure \n",
      "and other business units using AWS, a coordinated \n",
      "strategy is needed \n",
      "• Mitigation of lock-in and cost risks, particularly \n",
      "concerning Databricks, are needed.\n",
      "• A systematic approach to governance is essential, \n",
      "including the publication of use cases, identification of \n",
      "data ownership, and the development of data products.\n",
      "• There is a need for improved integration with the \n",
      "application and operational-oriented arch types, \n",
      "especially for low-latency use cases running on AWS.\n",
      "• Integration patterns with data-intensive applications will \n",
      "be crucial for maximizing the effectiveness of AI \n",
      "initiatives.Storage ADLS Gen 2\n",
      " Azure SQL DB\n",
      " PostgreSQL\n",
      "(AWS) \n",
      "35\n",
      "Sources Data usageTransform and ProcessExtract / Ingest\n",
      "Structured data\n",
      "Semi-structured data\n",
      "Unstructured data\n",
      "Output\n",
      "Data\n",
      "Data Processing\n",
      "Automation\n",
      "Delta Live Tables\n",
      "On-demand/Analyses processing\n",
      "Batch\n",
      "Data Intelligence Engine\n",
      "SRC\n",
      "SRC\n",
      "CRM\n",
      "JSON\n",
      "Streaming\n",
      "Data Management\n",
      "Serverless\n",
      "Lakehouse federation\n",
      "Job Clusters Workflow\n",
      "Predictive IO\n",
      "All-purpose \n",
      "Cluster\n",
      "Serverless SQL \n",
      "warehouse \n",
      "Predictive optimization\n",
      "Unity Catalog ODS\n",
      "Collibra\n",
      "Data Architecture\n",
      "Talend\n",
      "SilverBronze Gold\n",
      "Data Platform Governance\n",
      "Row/Column level security Catalog & Lineage\n",
      "Volumes Delta Tables\n",
      "Data Exchange\n",
      "Data Platform\n",
      "Structured data\n",
      "SRC\n",
      "SRC\n",
      "CRM\n",
      "Delta Sharing\n",
      "Code Assistant\n",
      "Access Control\n",
      "Streaming\n",
      "Confluent + \n",
      "Event Hub\n",
      "Spark Structured \n",
      "Streaming\n",
      "CDC\n",
      "Databricks \n",
      "connectors\n",
      "Data Quality in Databricks\n",
      "API\n",
      "APIM\n",
      "Data Platform Capabilities – Target Reference Implementation\n",
      "Proposed refinements for the reference architecture of International\n",
      "Fivetran connectors in \n",
      "Databricks\n",
      "JDBC connectors\n",
      "Machine Learning\n",
      "Feature store AutoMLMLflow\n",
      "Self-service & Sandboxing\n",
      "LAB environmentCloud sandboxPower Apps\n",
      "Data Analysis & Investigation\n",
      "MS Fabric or Databricks \n",
      "Notebooks & Dashboards\n",
      "AI/BI Genie space\n",
      "Reporting\n",
      "Databricks Dashboards\n",
      "Power BI (Fabric)\n",
      "Generative AI\n",
      "Low-code / no-code\n",
      "Agent to Agent Protocol\n",
      "Foundational models\n",
      "AI Vector Database\n",
      "Autoloader\n",
      "Application \n",
      "integration\n",
      "API Gateway\n",
      "Events\n",
      "Unstructured data processing\n",
      "MDM (master & reference \n",
      "data management) Responsible AI governance\n",
      "Metadata-driven \n",
      "Anonymization\n",
      "CDM\n",
      "Collibra –> Databricks \n",
      "integration\n",
      "MCP\n",
      "Azure DevOps DataOps MLOpsCI/CD Deployment Framework LLMOps\n",
      "Data-intensive \n",
      "applications\n",
      "Container \n",
      "services\n",
      "IntegrationProposed renewed Data & AI capability model \n",
      "36\n",
      "Data governance\n",
      "Strategy and use case \n",
      "management\n",
      "Data quality\n",
      "Data quality profiling \n",
      "Master Data \n",
      "Management\n",
      "Reference data \n",
      "management\n",
      "Data modelling and \n",
      "design\n",
      "Data value \n",
      "management\n",
      "Data ownership \n",
      "management\n",
      "Data principles & \n",
      "policy management\n",
      "Data awareness\n",
      "Data literacy\n",
      "Data ethics\n",
      "Data quality \n",
      "monitoring\n",
      "Data issue and \n",
      "remediation \n",
      "management\n",
      "DQ dashboarding\n",
      "Master data \n",
      "management\n",
      " Logical data modelling\n",
      "Physical data \n",
      "modelling\n",
      "Conceptual data \n",
      "modelling\n",
      "Citizen data scientist \n",
      "discovery, experiment \n",
      "and enablement (also \n",
      "includes no-/low-\n",
      "code GenAI\n",
      "Self-service business \n",
      "intelligence\n",
      "Managed business \n",
      "intelligence\n",
      "ML & Agentic \n",
      "development\n",
      "ML & Agentic training \n",
      "and fine-tuning\n",
      "ML & Agentic \n",
      "experiment tracking \n",
      "and model \n",
      "management \n",
      "ML & Agentic \n",
      "deployment\n",
      "ML & Agentic \n",
      "observability\n",
      "ML & Agentic \n",
      "inference \n",
      "management\n",
      "Data source \n",
      "management\n",
      "List of golden sources \n",
      "management\n",
      "Document & content \n",
      "management\n",
      "Data integration and \n",
      "interoperability\n",
      "Data sharing \n",
      "agreement \n",
      "management\n",
      "Data extraction and \n",
      "ingestion\n",
      "Data transforming\n",
      "Data orchestration \n",
      "and scheduling\n",
      "Data publication and \n",
      "exchange\n",
      "Data pipeline \n",
      "monitoring and \n",
      "observability\n",
      "Metadata \n",
      "management\n",
      "Lineage management\n",
      "Metamodel (and \n",
      "graph relationship) \n",
      "management\n",
      "Glossary and context \n",
      "managementData operations \n",
      "management\n",
      "Resilience & recovery\n",
      "Archiving\n",
      "Data life cycle \n",
      "management\n",
      "Data security\n",
      "Data anonymization, \n",
      "pseudonymization \n",
      "and synthetic data \n",
      "management\n",
      "Label, tag and \n",
      "classification \n",
      "management\n",
      "Data access \n",
      "management\n",
      "Consent management\n",
      "Data platform \n",
      "management\n",
      "Platform observability \n",
      "and monitoring\n",
      "Platform integration \n",
      "testing\n",
      "(Business) rule \n",
      "management\n",
      "Data product and \n",
      "asset management\n",
      "Compliance \n",
      "management\n",
      "Data streaming & \n",
      "Streaming analytics\n",
      "Conversational and \n",
      "chatbot management\n",
      "Vector and \n",
      "embedding \n",
      "management\n",
      "Data hub function\n",
      "Operational data \n",
      "processing (ODS)Best-of-suite versus best-of-breed considerations\n",
      "• Databricks is multi-cloud. \n",
      "• Databricks uses open-source standards, like Delta, Spark, MLflow, Unity \n",
      "Catalog, etc.\n",
      "• Databricks is tightly integrated into the Azure ecosystem and therefore \n",
      "facilitating strong security (usage of service principles, AD groups, key vault, \n",
      "etc.). This integration isn’t present in any (open-source) alternatives. \n",
      "• Integrating lots of components yourself is a daunting task.\n",
      "• Often lower total ownership costs due to bundling of services.\n",
      "• Uniform user experience and interface. Faster time to market.\n",
      "• Easier to hold one vendor accountable for support and updates.\n",
      "• Dependency on a single vendor can pose risks. Must be mitigated by not \n",
      "using too much high-risk lock-in services.\n",
      "• Not all necessary functionalities may be available, so still some lightweight \n",
      "integration might be required. This should be weighted carefully against \n",
      "extra effort and costs.\n",
      "37\n",
      "Databricks Data & AI Architecture\n",
      "ML + Data-oriented agents\n",
      "Application-oriented agents\n",
      "Proposed future-state reference architecture for data & AI – scenario #1\n",
      "38\n",
      "Two run-time stacks for AI Agents: Kubernetes (K8s) and Databricks\n",
      "Integration layer\n",
      "API Gateway (WSO2)\n",
      " Event Streaming (Kafka + Flink)\n",
      "Operational Data Stores (PostgreSQL, Azure SQL, Lakebase)\n",
      "Governance services\n",
      "Data Intelligence Platform (Databricks)\n",
      "Databricks \n",
      "Apps\n",
      "Foundation \n",
      "model APIs\n",
      "Mlflow\n",
      "(model governance)\n",
      "AI \n",
      "Gateway\n",
      "AutoML\n",
      "AI \n",
      "Playground\n",
      "Mosaic AI Agent Framework\n",
      "(tooling for development, build and deploy)\n",
      "Model \n",
      "Serving\n",
      "Vector \n",
      "Search\n",
      "Agent Bricks\n",
      "Foundational Models, such \n",
      "as OpenAI & Anthropic\n",
      "Source system\n",
      "Data layer\n",
      "Application \n",
      "layer\n",
      "Inference runtime \n",
      "environment\n",
      "(Kubernetes)\n",
      "Agents & model \n",
      "instances\n",
      "Monitoring & \n",
      "managementZelda IDP\n",
      "Lightweight Python \n",
      "dev environment for \n",
      "developing \n",
      "operational-aligned \n",
      "agents\n",
      "Base images with \n",
      "Python libraries\n",
      "AI agent \n",
      "development, \n",
      "testing, deployment\n",
      "Tracking \n",
      "and tracing\n",
      "Data Analytics platform (Databricks)\n",
      "Lakehouse \n",
      "(Medallion design with Delta \n",
      "+ Iceberg for both structured \n",
      "and unstructured data)\n",
      "Spark (data \n",
      "processing & \n",
      "ML)\n",
      "Databricks \n",
      "SQL\n",
      "Databricks \n",
      "Dashboards\n",
      "PowerBI\n",
      "Databricks \n",
      "Lakeflow\n",
      "Low-code*\n",
      "Low-code\n",
      "* Recognize that various platforms (e.g., Salesforce, SAP, Adobe, Databricks, Microsoft 365 & Teams, Collibra) are increasingly incorporating pay-per-use \n",
      "low-code features tailored to their specific services. Overseeing this trend, we should utilize the embedded agents (configured via low-code features) \n",
      "offered by our chosen platforms to optimize development within specific contexts. \n",
      "Lakebase\n",
      "Collibra\n",
      " Unity Catalog\n",
      "Responsible AI (TBD)\n",
      "AI Collaborative Marketplace (TBD)\n",
      "Data Quality (TBD)\n",
      "Low-code\n",
      "Extra embedding, RAG, \n",
      "MCP services, etc.\n",
      "Low-code\n",
      "AI Foundry voorstellen, voor \n",
      "webrtc, fine-tuning, complexe \n",
      "security (agent ID)\n",
      "Agentic development: Copilot Studio + \n",
      "VSCode\n",
      "Copilot Studio + \n",
      "VSCode voor AI \n",
      "development GAIA vervangt door \n",
      "Copilot Studio + \n",
      "RAG (AI Search)\n",
      "• Low-latency voice live APIs. Sanne GPT? Web RTC. OCR, \n",
      "Gaat niet binnen Databricks werken.\n",
      "• Fine-tuning. Microsoft heeft meerdere fine-tune opties. \n",
      "Onwaarschijnlijk dat dit binnen Databricks werkt.\n",
      "• Kosten zorg. Databricks komt met veel meer overhead en \n",
      "extra DBU kosten.\n",
      "• Complexe security (Microsoft AI safetify framework)\n",
      "• Agent ID (active directory, passthrough identity waarmee \n",
      "achterliggende agents weer data kunnen ophalen).\n",
      "• Copilot Studio ook binnen deze scope?\n",
      "• Als vervanging van GAIA, en dan scope iets oprekken naar \n",
      "Teams, SharePoint, Microsoft 365, etc.\n",
      "• Open Telemetry ondersteuning in Databricks\n",
      "• AI Foundry gaat straks samen op met Azure ML Services.\n",
      "Agent \n",
      "store\n",
      "Employee productivity\n"
     ]
    }
   ],
   "source": [
    "print(architecture_proposal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_template = \"\"\"---\n",
    "title: ADR Template\n",
    "---\n",
    "\n",
    "---\n",
    "MADR template version: 3.0.0 \n",
    "MADR template source: https://github.com/adr/madr\n",
    "---\n",
    "\n",
    "# ADR Template\n",
    "\n",
    "## Title. \n",
    "*mandatory*  \n",
    "The title assigns a {name} to the AD so that it can be identified and searched for easily. Ideally, it should convey the essence of the problem solved and the solution chosen.\n",
    "\n",
    "## Metadata.\n",
    "*mandatory*  \n",
    "\n",
    "The metadata elements are:\n",
    "\n",
    "| Attribute Name | Description | Format |mandatory optional|\n",
    "| -------------- | ----------- | ------ |------|\n",
    "| status | status of the architecture decision | proposed \\\\| accepted \\\\| rejected \\\\| deprecated \\\\| superseded by | mandatory |\n",
    "| date | when the decision was last updated | {YYYY-MM-DD} | mandatory |\n",
    "| deciders | lists everyone involved in the decision |  | mandatory |\n",
    "| consulted | lists everyone whose opinions are sought and with whom there is a two-way communication |  | optional |\n",
    "| informed | lists everyone who is kept up-to-date on progress in one-way communication |  | optional |\n",
    "\n",
    "### NN additions  \n",
    "*mandatory*\n",
    "\n",
    "| Attribute Name | Description | Format |mandatory optional|\n",
    "| -------------- | ----------- | ------ |------|\n",
    "| Identifier | Unique identifier of the decision | < integer > | mandatory |\n",
    "| Organizational Scope | Identification of the organizational scope to which the architecture decision applies. This scope comprises of one or more predefined organizational units. | NN Group \\\\| < country > \\\\| < BU > \\\\| < country >\\\\\\\\< BU > \\\\| < country >\\\\\\\\< BU >\\\\\\\\< LoB > | mandatory |\n",
    "| Functional Scope | Identification of the functional scope to which the architecture decision applies. This scope comprises of one or more domains, subdomains or digital products, preferably predefined, but free text is acceptable as a minimal viable solution. | < domain > \\\\| < domain >\\\\\\\\< subdomain > \\\\| < domain >\\\\\\\\< subdomain >\\\\\\\\< digital product > | mandatory |\n",
    "\n",
    "## Context and Problem Statement.  \n",
    "*mandatory*  \n",
    "Describes the context and problem statement in a few sentences. One may want to articulate the problem in form of a question or provide an illustrative story that invites to a conversation. Links to collaboration boards or issue management systems can go here too.\n",
    "\n",
    "## Decision Drivers.  \n",
    "*optional*  \n",
    "Desired qualities, forces, faced concerns are identified here:\n",
    "\n",
    "- {decision driver n}\n",
    "\n",
    "## Considered Options.  \n",
    "*mandatory*  \n",
    "This section lists the alternatives (or choices) investigated:\n",
    "\n",
    "- {title/name of option n}\n",
    "\n",
    "The template recommends listing the chosen option first (as a project-wide convention). One needs to make sure to list options that can solve the given problem in the given context (as documented in Section “Context and Problem Statement”). They should do so on the same level of abstraction; a mistake we have seen in practice is that a technology is compared with a product, or an architectural style with a protocol specification and its implementations. Pseudo-alternatives sometimes can be found too, but do not help.\n",
    "\n",
    "## Chosen Option.  \n",
    "*mandatory*  \n",
    "Here, the chosen option is referred to by its title. A justification should be given as well: {name of option 1} because {justification}. Some examples of justifications are: it is the only option that meets a certain k.o. criterion/decision driver; it resolves a particular force; it comes out best when comparing options. \n",
    "\n",
    "## Consequences.  \n",
    "*mandatory*  \n",
    "This section discusses how problem and solution space look like after the decision is made (and enforced).\n",
    "\n",
    "Positive and negative consequences are listed as “Good, because …” and “Bad, because …”, respectively. An example for a positive consequence is an improvement of a desired quality. A negative consequence might be extra effort or risk during implementation.\n",
    "\n",
    "## Validation.  \n",
    "*optional*  \n",
    "This optional section describes how the implementation of/compliance with the ADR is evaluated (aka enforced), for instance by way of a review or a test. See “A Definition of Done for Architectural Decision Making” for related hints.\n",
    "\n",
    "## Pros and Cons of the Options.  \n",
    "*mandatory*  \n",
    "Here, the alternatives that address the problem can be explained and analyzed more thoroughly.\n",
    "\n",
    "The template advises to provide an example or a description of the option. Then, “Good” and “Bad” options properties are asked for. For noteworthy “Neutral” arguments, the template suggests the form\n",
    "\n",
    "Neutral (w.r.t.), because {argument}.\n",
    "\n",
    "## More Information.  \n",
    "*optional*  \n",
    "Here, one might want to provide additional evidence for the decision outcome (possibly including assumptions made) and/or document the team agreement on the decision (including the confidence level) and/or define how this decision should be realized and when it should be re-visited (the optional “Validation” section may also cover this aspect). Links to other decisions and resources might appear in this section as well.\n",
    "\n",
    "Include a reference to the Design Doc that this ADR has originated from. While the ADR is lightweight, the design doc contains more detailed descriptions and diagrams.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are an experienced enterprise architect. You assist in writing Architecture Decision Records (ADRs) \\\n",
    "Given the following context, generate an Architecture Decision Record (ADR) in Markdown format. Include the problem, decision, status, alternatives considered, pros/cons, and consequences. \\\n",
    "You are provided via the chat with instructions about what to write the ADR for. \\\n",
    "You must adhere to the structure of the following template: {adr_template}\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Architecture document:\\n{architecture_proposal}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user.\"\n",
    "\n",
    "system_prompt += f\"Crucial Rules:\\\n",
    "- You must allow the user to make revisions on the ADR you write.\\\n",
    "- You must ask the user whether the ADR is ready to be handed off, and if so, you must hand it off to the store_adr tool.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an experienced enterprise architect. You assist in writing Architecture Decision Records (ADRs) Given the following context, generate an Architecture Decision Record (ADR) in Markdown format. Include the problem, decision, status, alternatives considered, pros/cons, and consequences. You are provided via the chat with instructions about what to write the ADR for. You must adhere to the structure of the following template: ---\\ntitle: ADR Template\\n---\\n\\n---\\nMADR template version: 3.0.0 \\nMADR template source: https://github.com/adr/madr\\n---\\n\\n# ADR Template\\n\\n## Title. \\n*mandatory*  \\nThe title assigns a {name} to the AD so that it can be identified and searched for easily. Ideally, it should convey the essence of the problem solved and the solution chosen.\\n\\n## Metadata.\\n*mandatory*  \\n\\nThe metadata elements are:\\n\\n| Attribute Name | Description | Format |mandatory optional|\\n| -------------- | ----------- | ------ |------|\\n| status | status of the architecture decision | proposed \\\\| accepted \\\\| rejected \\\\| deprecated \\\\| superseded by | mandatory |\\n| date | when the decision was last updated | {YYYY-MM-DD} | mandatory |\\n| deciders | lists everyone involved in the decision |  | mandatory |\\n| consulted | lists everyone whose opinions are sought and with whom there is a two-way communication |  | optional |\\n| informed | lists everyone who is kept up-to-date on progress in one-way communication |  | optional |\\n\\n### NN additions  \\n*mandatory*\\n\\n| Attribute Name | Description | Format |mandatory optional|\\n| -------------- | ----------- | ------ |------|\\n| Identifier | Unique identifier of the decision | < integer > | mandatory |\\n| Organizational Scope | Identification of the organizational scope to which the architecture decision applies. This scope comprises of one or more predefined organizational units. | NN Group \\\\| < country > \\\\| < BU > \\\\| < country >\\\\\\\\< BU > \\\\| < country >\\\\\\\\< BU >\\\\\\\\< LoB > | mandatory |\\n| Functional Scope | Identification of the functional scope to which the architecture decision applies. This scope comprises of one or more domains, subdomains or digital products, preferably predefined, but free text is acceptable as a minimal viable solution. | < domain > \\\\| < domain >\\\\\\\\< subdomain > \\\\| < domain >\\\\\\\\< subdomain >\\\\\\\\< digital product > | mandatory |\\n\\n## Context and Problem Statement.  \\n*mandatory*  \\nDescribes the context and problem statement in a few sentences. One may want to articulate the problem in form of a question or provide an illustrative story that invites to a conversation. Links to collaboration boards or issue management systems can go here too.\\n\\n## Decision Drivers.  \\n*optional*  \\nDesired qualities, forces, faced concerns are identified here:\\n\\n- {decision driver n}\\n\\n## Considered Options.  \\n*mandatory*  \\nThis section lists the alternatives (or choices) investigated:\\n\\n- {title/name of option n}\\n\\nThe template recommends listing the chosen option first (as a project-wide convention). One needs to make sure to list options that can solve the given problem in the given context (as documented in Section “Context and Problem Statement”). They should do so on the same level of abstraction; a mistake we have seen in practice is that a technology is compared with a product, or an architectural style with a protocol specification and its implementations. Pseudo-alternatives sometimes can be found too, but do not help.\\n\\n## Chosen Option.  \\n*mandatory*  \\nHere, the chosen option is referred to by its title. A justification should be given as well: {name of option 1} because {justification}. Some examples of justifications are: it is the only option that meets a certain k.o. criterion/decision driver; it resolves a particular force; it comes out best when comparing options. \\n\\n## Consequences.  \\n*mandatory*  \\nThis section discusses how problem and solution space look like after the decision is made (and enforced).\\n\\nPositive and negative consequences are listed as “Good, because …” and “Bad, because …”, respectively. An example for a positive consequence is an improvement of a desired quality. A negative consequence might be extra effort or risk during implementation.\\n\\n## Validation.  \\n*optional*  \\nThis optional section describes how the implementation of/compliance with the ADR is evaluated (aka enforced), for instance by way of a review or a test. See “A Definition of Done for Architectural Decision Making” for related hints.\\n\\n## Pros and Cons of the Options.  \\n*mandatory*  \\nHere, the alternatives that address the problem can be explained and analyzed more thoroughly.\\n\\nThe template advises to provide an example or a description of the option. Then, “Good” and “Bad” options properties are asked for. For noteworthy “Neutral” arguments, the template suggests the form\\n\\nNeutral (w.r.t.), because {argument}.\\n\\n## More Information.  \\n*optional*  \\nHere, one might want to provide additional evidence for the decision outcome (possibly including assumptions made) and/or document the team agreement on the decision (including the confidence level) and/or define how this decision should be realized and when it should be re-visited (the optional “Validation” section may also cover this aspect). Links to other decisions and resources might appear in this section as well.\\n\\nInclude a reference to the Design Doc that this ADR has originated from. While the ADR is lightweight, the design doc contains more detailed descriptions and diagrams.\\n\\n\\n## Architecture document:\\nRevision History\\nVersion Revision date Status Summary of changes By\\n0.1 Concept Initial version Piethein Strengholt\\n0.2 26-05-2026 Concept Second version Discussed with Rob Visser, Tjerrie Smit, Sonja Miljoen, John van Iersel, Arno Goosen\\n0.2 26-05-2026 Concept Incorporating feedback from PWC and International stakeholders Informed: Bob Zondervan, Adrian Matei, Imre Sztanó, Frank Eijsink\\n0.3 12-06-2025 Concept Revised different scenarios, added extra requirements Review Jack Jonathans\\n0.4 16-06-2025 For information Discussed with McKinsey, Katalin Tatar\\n0.5 26-06-2025 Review Feeback WS2 working group Jack Jonathans, Marcel van Dijk, Arno Goosen, Sonja Miljoen, Hein Peters\\n0.6 26-06-2025 For information Informed: Wouter Wijnmalen, Maarten de Regt, Laurens van Beurden, Jorik Blaas-Sigmond\\n0.7 04-07-2025 Discussion Discussed with Tjeerd Bosklopper, Maurice Koopman, Guido Bosch, Rob Visser, Tjerrie Smit\\n0.8 16-07-2025 Discussion Changes for TPAB Discussed within TPAB\\nDocument ManagementMission Statement for NN's Data Strategy\\nAspiration\\n2\\nOur North Star is to be a digitally empowered organization that places data at the heart \\nof our decision-making processes\\n• NN is committed to be front runner on the data & ai insurance market.\\n• International, for example, is seeing the potential of at least €150 mil. with Agentic AI\\nCurrent \\nsituation\\n• Inspired by the Seattle trip, NN is dedicated to embark on the Agentic AI journey.\\n• Non-life is ready to engage now with Quantum Black for AI use cases for claims and \\nunderwriting.\\n• NN International is ready to engage now with PWC for the data platform and Salesforce & \\nAccenture for the customer service use cases.\\n• AI is evolving at an incredible speed. Scaling the current architecture is challenging.\\nRecommendation\\n• We need to establish a solid foundation for scaling AI throughout the organization.\\n• We want to learn from our mistakes, like diversity of different NNDAP platforms. With \\nstrong discipline at the core, we would like to do it right from the beginning.Current landscape\\n3\\nAbstract view of our today’s architecture\\nUser interfaces Reports\\nSQL\\nAnalytical platforms\\n(structured data only)\\nTraining\\nProcess- or  \\napplication layer\\n(business logic and orchestration)\\nAPIs (CRUD)\\nOperational systems\\nML\\nRuntime\\nCurrent situation:\\nVaried human-based experiences: Today, the interaction is often \\ncharacterized by a traditional approach, where end-users \\npredominantly rely on interfaces rather than autonomous \\nsystems. The engagement is typically supportive, involving the \\ngeneration of reports derived from the analytical chain.\\nProcess orchestration: This is generally rule-based and closely \\nintegrated with applications and software systems.\\nAnalytical data: Platforms typically concentrate on structured, \\nbatch-driven data.\\nMachine Learning relies on analytical systems for data processing \\nand training. These systems play a crucial role in preparing and \\nrefining the data needed for effective model development. \\nHowever, the operationalization of the analytical models primarily \\noccurs on the operational side.How will applications be transformed with Agentic AI?\\n4\\nAgentic AI refers to an artificial intelligence system designed to achieve specific goals with minimal human \\nsupervision\\nThe impact of artificial intelligence (AI) on modern \\napplications is profound and multifaceted, transforming \\nthe way software interacts with users and processes \\ninformation. As AI continue to evolve, they are \\nincreasingly integrated into various applications, \\nenhancing their capabilities and creating new \\nopportunities for innovation.\\nAt the core of this transformation is the concept of AI \\nagents—autonomous or semi-autonomous entities that \\ncan perform tasks, make decisions, and learn from their \\nenvironment. These agents work with Large Language \\nModels, retrieve data, and can be orchestrated through \\nframeworks like LangChain, LangGraph, and CrewAI, \\nwhich enable developers to create sophisticated \\napplications that leverage the power of AI.\\nAgent \\norchestration\\nApplication\\nContext\\nLLM\\nTools\\nAgent\\nAgent\\nAgent\\nFrameworks like \\nLangChain, LangGraph, \\nCrewAI\\nOrchestration and decisioningFuture state: how AI is impacting the overall landscape\\n5\\nAbstract view of our future architecture\\nFuture situation:\\nEmpowering users with contextual intelligence: Leverage contextual data to \\ndeliver actionable insights and recommendations, transforming user \\nexperiences and driving productivity.\\nAgentic Intelligence: Move beyond rule-based automation to contextually \\naware systems that adapt to real-time changes, enhancing operational \\nefficiency.\\nContext-Aware Interactions: Integrate context-aware protocols like the Model \\nContext Protocol (MCP) to enable applications that intelligently respond.\\nUnstructured data: AI's ability to process both structured and unstructured \\ndata (documents, log files, images, videos, and so on) for richer insights, driving \\ninformed decision-making across the organization. \\nGenerative AI is poised to infiltrate both operational and analytical side of the architecture. \\nConsequently, agents must be deployed different environments. \\nTo fully leverage the capabilities of AI, it is essential that operational systems are exposed \\nthrough modern APIs. Furthermore, a careful review and redefinition of business processes is \\nessential to ensure that organizations can effectively harness the power of Agentic AI.\\nLastly, without reliable and well-structured data, the effectiveness of AI applications can be \\nsignificantly diminished.\\nConversational layer\\n(interfaces for voice, web, search, instructions, video, etc.) Gen BI\\nContext-aware layer\\nAPIs (CRUD)\\nOperational systems\\nML Runtime\\nContext-aware layer\\nAnalytical platforms\\n(structured + unstructured)\\nSQLGenAITraining\\nAgentic layer\\n(query resources, execute tasks, perform reflections, orchestrate, etc.)\\nAgent runtime Agent runtimeWhere are we today?\\n• NNDAP v4.x Adoption Issues: The current version of NNDAP v4.x lacks broad adoption. A move to a layered \\narchitecture (minimal platform core L1, expanded platform L2, and extended platform L3) is necessary, \\nrequiring migrations to NNDAP v5.x L1.\\n• Lack of ODS and Unstructured Data Support: NNDAP does not support Operational Data Store (ODS) \\nfunctionality and lacks support for unstructured data, both of which need architectural decisions for \\nresolution.\\n• NNDIP Adoption Challenges: NNDIP has not been widely adopted, potentially due to the strong focus on \\nMLflow and lack of self-service and automation.\\n• GAIA Architecture: The GAIA architecture lacks clear definitions and product management.\\n• Collibra and Unity Catalog Issues: The issues of Collibra and Unity Catalog large stem from a lack of \\ncollaboration within NN and insufficient knowledge across the team. Moreover, we lack the skills to set up \\nplatforms effectively at an enterprise level, focusing too much on minor differences rather than \\ncommonalities.\\n• Need for AI Governance Replacement: The SECA for AI Governance is deemed insufficient and requires \\nreplacement with better solutions like Collibra, Monitaur, or ServiceNow AI Control Tower.\\n6\\nEnterprise architecture should have \\na stronger leading voice in setting \\nthe direction for the future Data & \\nAI architecture. The current \\ncapabilities are mostly developed \\napart from the other capabilities.\\nA comprehensive end-to-end \\narchitecture is lacking, which is \\nhindering efficient alignment \\nacross different teams.\\nDiverse technology stack and varied maturity levels are hindering NN’s scaling ambitionsGAIA\\nBreak down of the current-state data architecture\\n7\\nSource system\\nIntegration layer\\nAPI Gateway\\n Event Streaming (work-in-progress)\\nOperational Data Stores\\nGovernance Layer\\nNNDAP\\nLakehouse \\n(Medallion design with Delta)\\nSpark \\n(data processing \\n& ML)\\nAbstraction \\nlayer\\nHosting of Vector \\ndatabases\\nWeb-interface \\nRuntime \\nenvironment\\n(Kubernetes)\\nAgent \\ninstances\\nNNDIP\\nSeldon\\n(Model \\nmanagement)\\nOther local platforms for \\nanalytics and AI projects \\n(e.g., Dataiku)\\nExternal \\nFoundational \\nmodels\\n(OpenAI)\\nLow-code\\n(missing)\\n(Local) dev \\nenvironments\\nOnboarding of unstructured \\ndata by GAIA team\\nAbstraction \\nlayer\\nThe current technology stack varies based on use case requirements like RAG and ML\\nAzure Data \\nFactory\\nData layer\\nApplication \\nlayer\\n PowerBI\\nCollibra\\n Unity Catalog\\nResponsible AI (missing)\\n Data quality (missing)Capabilities need for scaling data & AI\\n8\\nThese building blocks must work together systematically (not piecemeal) to support \\nscaled, safe AI use\\nGovernance\\n(for both data and \\nresponsible AI)\\nMarketplace \\nfor data product and \\nAI-related assets\\nAgentic AI \\ncapability\\nData engineering & \\nreporting capability\\nData quality \\nmanagement\\nAgentic AI is the focus of this presentation. However, the \\nsurrounding capabilities play a very important role for Agentic AI. \\nTherefore, we also propose changes to other related capabilities.9\\nWhat NNDIP + GAIA cover well:\\n• Hosts both classical ML and GenAI models (LLMs)\\n• Supports deployment of pre-processing transformers, post-processing logic, and external service \\nintegration (with help from the central team)\\n• Provides functional monitoring, including drift detection, outlier detection, and latency tracking \\nvia Seldon\\n• Enforces deployment conventions for governance, reuse, and lifecycle traceability\\n• Allows onboarding of unstructured data (no reusable data lake yet)\\n• Each pipeline is tailored to specific use cases\\nWhat NNDIP + GAIA lack for scalable agentic AI:\\n• No managed development environment or integrated AI playground\\n• No baseline image with boilerplate code, scaffolded folder structures, scripts, and frameworks \\nfor various AI use cases\\n• Lacks built-in support for persistent memory or context retention across requests\\n• No paved road experience (e.g., quickly creating repositories, CI/CD pipelines, API endpoints, \\netc.)\\n• Lacks mechanisms for agentic planning, complex orchestration, and communication buses (e.g., \\nMCP servers, integration with API gateways, and event platforms)\\n• Misses several AI governance (e.g., fallback handling), prompt governance (e.g., drift), cost \\ngovernance (e.g., API chain costs), embedded security & access governance, ethical/compliance\\nHeatmap of the maturity of NN’s Agentic AI capability\\nLevel 1: Obsolete / Missing\\n Level 2: Developing Maturity\\n Level 3: Acceptable Maturity\\nArchitecture of the agentic ecosystem and maturity assessment of current-state\\nBase platform\\nKnowledge management services\\nDeveloper \\nservices\\nAgentic engineering \\nexperience via \\nInternal Developer \\nPlatform (IDP)\\nAI playground and \\ndata self-service \\nenvironment\\nManaged AI \\ndeveloper \\nplatform\\nAgentic services\\nInteraction and integration services\\nManaged runtime \\n(Kubernetes)\\nBase images \\n(pre-selected frameworks, \\nscaffolded folder structures)\\nRAG (Vector and \\nEmbedding services)\\nShort-term storage\\n(cache)\\nRetrieval services (web \\ncrawl)\\nDocument parse and \\nembedding services\\nOffering MCP services\\nAccess to Foundational \\nModels\\nCognitive services\\nFine-tuning\\nIntegration services for APIs and events\\nExperience-related services for voice, video, search\\nGovernance\\nObservability\\nOperations\\nAI & ML model \\nmanagement\\nAI asset \\nmarketplace\\nResponsible AI \\nGovernance\\nResponsible AI \\nGovernance\\nMarketplace \\nfor data and \\nAI-related \\nassets\\nAgentic AI \\ncapability\\nData analytics \\n& reporting\\nData quality \\nmanagement\\nCertain level of automation and self-serviceBuilding and scaling our own technology stack is challenging\\n• Agentic AI Frameworks: The field of generative AI is advancing quickly, with new \\nframeworks, such as Langchain, LangGraph, Pydantic AI, Llamalndex, CrewAI, \\nAutoGen, and Semantic Kernel. Committing to a particular set of frameworks may \\nlimit your organization's ability to adapt to these advancements.\\n• LLM of the day: New LLMs and LLM updates are released frequently, with today's \\nreleases surpassing the champions of yesterday. So, we need to be flexible in a way \\nthat LLMs can be changed.\\n• Agent Protocols: Agent-to-agent and -application protocols like MCP , A2A, and ANP \\nhave emerged, with more on the way.\\n• Diverse needs: Projects have different requirements, such as the need for low-code \\nor no-code solutions for quick delivery or usage, while others may need pro-code \\napproaches for complex application integration scenarios. Balancing these options \\nenhances agility and responsiveness.\\n10\\nTo enhance the organization’s effectiveness \\nin leveraging generative AI, it is \\nrecommended to establish a Centralized \\nOversight Team. This team would be \\nresponsible for overseeing the latest \\ndevelopments, frameworks, tools, and best \\npractices across various use cases. By closely \\nmonitoring advancements in the field, \\nevaluating and selecting suitable \\nframeworks, and setting standardized best \\npractices, the team can quickly adapt the \\ndirection.\\nThe market is evolving with new AI products and features at an unprecedented paceSeveral AI patterns – there is no one size fits all\\n11\\nQuery LLM\\n(Generate)\\nLLM\\n(Evaluate)\\nOutputOutput\\nWeb\\nApp Response\\n•Invoking LLMs and APIs\\n•No external data usage\\nQuery Embed\\nprompt\\nLLM\\nqueryApp\\n•Invoking LLMs and APIs\\n•Requiring a vector database\\n•Executing Python scripts\\nVectors\\noutput\\ninput\\nResponse\\n2. Retrieval-Augmented Generation (RAG) pattern\\n1. Standalone agent pattern\\nQuery\\nMCP \\nServer\\nApp\\n•Requiring hosting of MCP Servers\\n•Interacting with external services\\nVectors\\nMCP \\nServer SaaS vendorAI application\\n(Agent)\\nMCP \\nClient\\nLLM\\nResponse\\nResponse\\nResponse\\nQuery\\nApp\\nLLM\\nResponse\\nResponse\\nquery\\nVectors\\noutput\\nState\\nAI application\\n(Agent)\\nAI application\\n(Agent)\\nAI application\\n(Agent)\\n•Running multiple agents \\n•Querying vector databases\\n•Orchestration and usage of a store state\\n4. Multi-agent pattern\\n3. Model Context Protocol (MCP) pattern\\nWe need to have a flexible architecture for facilitating different scenarios (this is not an exhaustive overview)\\nBase images* Managed runtime* Management*\\nRetrieval Document parse Embedding\\nOrchestration\\nMCP serversComplex application integration\\nComplex application integration\\nRetrieval Document parse Embedding\\n* Required for all scenariosRequirements\\n• Fast delivery of the new architecture to meet the demands of our organization (several teams already want to start in Q3 2025).\\n• Need for standardizing our IT foundations for tackling complexity and increased flexibility.\\n• Recognize the need for different architecture runways:\\n• Data engineering and/or machine learning for data-intensive use cases.\\n• AI or automation-related development in the context of improving operational processes.\\n• AI-related development aimed at providing rich context using large amounts of data.\\n• Managing governance to adhere to external regulations and internal standards.\\n• Recognize the need for different types of agents, such as operational agents that are context-aware and can operate with low latency in operational systems, as well as \\nanalytical agents that can serve processed datasets.\\n• This translates to the necessity of working with different runtime environments (e.g., Databricks and Kubernetes) and deploying across various cloud environments.\\n• A unified approach to structured and unstructured data management is essential, with data products that can be consumed quickly in other environments and the ability to \\nseamlessly use data across different platforms for machine learning and AI.\\n• We must remain flexible for the future by adopting open standards such as Delta, Iceberg, Spark, Kafka, and Flink, as well as open specifications like the Open Data Contracts \\nStandard and the Open Data Product Specification.\\n• Real-time data processing is critical\\n• There is a strong need for a single and simplified governance process for onboarding AI and hyper-automation use cases, registration of data products, registration of AI-related \\nartifacts, and more. This governance framework also encompasses agents that are integrated into SaaS solutions. It is important to enable every employee to generate value \\nwith AI, which necessitates the establishment of low-code standards. \\n• There is also a need for high-quality data, which requires a robust data quality capability.\\n• There is a strong desire for two different types of marketplaces: 1) one for collaborative co-creation of AI-related assets, and 2) one for certified data products, data quality, and \\nrelated services.\\n• Ensure interoperability between Business Units (BUs) for rapid data sharing and implement enterprise-wide data standardization to minimize redundant tasks for users.\\n12\\nHigh-level requirements that frame the future-state architectureComparing different types of Agents: application-oriented and data-oriented\\nApplication-oriented agents\\nAI agents designed to function within real-time operational environments, focusing on \\nimmediate tasks and interactions.\\n• Context-Aware: Capable of understanding and adapting to the current context in which \\nthey operate.\\n• Low Latency: Optimized for quick responses and actions, crucial for operational efficiency.\\n• System Interaction: Engage with various systems through APIs to retrieve or manipulate \\ndata as needed. Or require extra application components like a data-intensive application or \\nOperational Data Store (ODS).\\n• Real-Time Data Access: Able to fetch the most recent data to ensure decisions are based on \\nthe latest information.\\n• Long-Running Processes: May be involved in complex, ongoing workflows that require \\norchestration across multiple systems.\\n• Example use cases: Hyper-automation, real-time fraud detection agents, dynamic risk and \\npricing agents, contextual recommendation agents, real-time claims processing agents, and \\nso on.\\n13\\nThe importance of recognizing agent types\\nData-oriented agents\\nAI agents focused on data processing, enriching, and analyzing data to derive insights and \\nsupport decision-making.\\n• Insight Generation: Provide analytical outputs that help in understanding patterns, \\nforecasting, and making informed decisions.\\n• Data Processing: Typically handle larger data sets in a batch manner rather than in \\nreal-time, allowing for comprehensive analysis.\\n• Limited strong consistency requirements: Unlike operational agents, they may not \\nrequire most accurate data for real-time data or transactional processing. Instead, \\nthese type of agents more typically for on deeper analytical tasks. So, latency is \\naccepted to be higher.\\n• Example use cases: Chatting with your own data, data enrichment agents, serving a \\nhistorical profile, customer 360, big data processing agents, market research analysis \\nagents, finance controlling agents, policy renewal prediction agents, regulatory \\ncompliance monitoring agents, etc.\\nML + Data-oriented agents\\nApplication-oriented agents\\nProposed future-state reference architecture for data & AI – scenario #1\\n14\\nTwo run-time stacks for AI Agents: Kubernetes (K8s) and Databricks\\nIntegration layer\\nAPI Gateway (WSO2)\\n Event Streaming (Kafka + Flink)\\nOperational Data Stores (PostgreSQL, Azure SQL, Lakebase)\\nGovernance services\\nData Intelligence Platform (Databricks)\\nDatabricks \\nApps\\nFoundation \\nmodel APIs\\nMlflow\\n(model governance)\\nAI \\nGateway\\nAutoML\\nAI \\nPlayground\\nMosaic AI Agent Framework\\n(tooling for development, build and deploy)\\nModel \\nServing\\nVector \\nSearch\\nAgent Bricks\\nFoundational Models, such \\nas OpenAI & Anthropic\\nSource system\\nData layer\\nApplication \\nlayer\\nInference runtime \\nenvironment\\n(Kubernetes)\\nAgents & model \\ninstances\\nMonitoring & \\nmanagementZelda IDP\\nLightweight Python \\ndev environment for \\ndeveloping \\noperational-aligned \\nagents\\nBase images with \\nPython libraries\\nAI agent \\ndevelopment, \\ntesting, deployment\\nTracking \\nand tracing\\nData Analytics platform (Databricks)\\nLakehouse \\n(Medallion design with Delta \\n+ Iceberg for both structured \\nand unstructured data)\\nSpark (data \\nprocessing & \\nML)\\nDatabricks \\nSQL\\nDatabricks \\nDashboards\\nPowerBI\\nDatabricks \\nLakeflow\\nLow-code*\\nLow-code\\n* Recognize that various platforms (e.g., Salesforce, SAP, Adobe, Databricks, Microsoft 365 & Teams, Collibra) are increasingly incorporating pay-per-use \\nlow-code features tailored to their specific services. Overseeing this trend, we should utilize the embedded agents (configured via low-code features) \\noffered by our chosen platforms to optimize development within specific contexts. \\nLakebase\\nCollibra\\n Unity Catalog\\nResponsible AI (TBD)\\nAI Collaborative Marketplace (TBD)\\nData Quality (TBD)\\nLow-code\\nExtra embedding, RAG, \\nMCP services, etc.\\nLow-codeProposed future-state reference architecture for data & AI – scenario #2\\n15\\nDatabricks as a best-of-suite platform for all data and AI use cases (AWS and Azure)\\nSource system\\nData layer\\nApplication \\nlayer\\nIntegration layer\\nAPI Gateway\\n Event Streaming (Kafka + Flink)\\nOperational Data Stores\\nDeploy and run on \\nother environments, \\nsuch as AWS\\nInference runtime \\nenvironment\\n(Kubernetes)\\nAgent instances\\nInference outside DBX\\nData Analytics platform (Databricks)\\nLakehouse \\n(Medallion design with Delta + \\nIceberg for both structured and \\nunstructured data)\\nSpark \\n(data processing \\n& ML)\\nDatabricks \\nSQL\\nDatabricks \\nDashboards\\nDatabricks \\nLakeflow\\nPowerBI\\nData Intelligence Platform (Databricks)\\nDatabricks \\nApps\\nFoundation \\nmodel APIs\\nMLflow\\nModel \\nServing\\nAutoML\\nAI \\nPlayground\\nMosaic AI Agent Framework\\nAI \\nGateway\\nVector \\nSearch\\nAgent Bricks\\n(low-code)\\nFoundational Models, such \\nas OpenAI & Anthropic\\nLakebase\\nGovernance services\\nCollibra\\n Unity Catalog\\nResponsible AI\\nAI Marketplace (TBD)\\nData Quality (TBD)Proposed future-state reference architecture for data & AI – scenario #3\\n16\\nBest of breed approach (upscaling the existing architecture)\\nIntegration layer\\nAPI Gateway\\n Event Streaming (Kafka + Flink)\\nOperational Data Stores\\nRuntime environment\\n(Kubernetes)\\nData-intensive \\napplications\\nAgent instances\\nZelda IDP\\nBase images \\nwith  Python \\nlibraries\\nAI development, \\ntesting, \\ndeployment\\nTracking and \\ntracing\\nNNDIP\\nSeldon\\n(Model \\nmanagement)\\nOther local \\nplatforms for \\nanalytics and \\nAI projects \\n(e.g., Dataiku)\\nGAIA\\nFoundational \\nModels, such as \\nOpenAI & \\nAnthropic\\nAbstraction layer\\nHosting of Vector \\ndatabases\\nWeb-interface \\n Low-code\\n(TBD)\\nSource system\\nData layer\\nApplication \\nlayer\\nNNDAP\\nLakehouse \\n(Medallion design with Delta + \\nIceberg for both structured \\nand unstructured data)\\nSpark (data \\nprocessing & \\nML)\\nDatabricks \\nSQL\\nDatabricks \\nDashboards\\nPowerBI\\nDatabricks \\nLakeflow\\nLakebase\\nGovernance services\\nCollibra\\n Unity Catalog\\nResponsible AI\\nAI Marketplace (TBD)\\nData Quality (TBD)\\nServing context and \\nLLM interactionConsiderations for all three scenarios\\nPros:\\n• Reduces infrastructure complexity and associated costs by using a \\nsingle platform.\\n• Faster time to market: A best-of-suite approach accelerates time to \\nmarket by providing integrated, streamlined solutions that simplify \\ndeployment, reduce complexity, and enhance user experience.\\n• Simplified governance and guidance to all teams.\\n• High level of trust. Communities and guilds are already actively and \\nenthusiastically engaging with Databricks.  \\nCons:\\n• Databricks also must be hosted on AWS.\\n• NNDIP and GAIA must be re-designed/re-platformed.\\n• Databricks will be used for all types of workloads, including \\nlightweight tasks that may not require its full capabilities. This could \\nlead to suboptimal utilization of resources and features .\\n• May not accommodate specialized edge cases that require \\nalternative deployment patterns. For instance, high-latency use cases \\nmay not perform optimally if platform doesn't meet specific needs.\\n• Heavy reliance on a single provider, which can lead to challenges if \\nswitching platforms becomes necessary.\\n17\\n#2 Best-of-suite approach with Databricks \\n#1 Two run-time stacks for AI Agents: \\nKubernetes (K8s) and Databricks\\nIn this scenario, Azure Databricks serves as the comprehensive \\nplatform for all data and AI use cases. It simplifies development by \\nproviding integrated tools for data processing, machine learning, and \\ncollaboration. This approach is particularly cost-effective and well-\\nsuited for high-latency applications running within the Azure \\necosystem. However, it may fall short for certain edge cases that \\nnecessitate deploying models and agents on Kubernetes in AWS.\\nA dual approach might be more desired for more a specialized \\ndevelopment processes. This scenario employs a hybrid architecture \\nwhere Databricks (both AWS and Azure) is used for most data \\nintegration and AI development, while a separate tech stack utilizing \\nPython and Agentic frameworks is deployed for specific use cases. The \\nPython stack runs on containerized environments, potentially on \\nKubernetes, and integrates back to Azure Databricks for experiment \\nand model tracking and tracing via managed MLflow in Databricks. This \\nprovides flexibility for edge cases while retaining a strong central \\nregistry in Databricks.\\nThis scenario could be implemented organically, next to scenario #2, \\ndepending on results and specific use cases.\\nPros:\\n• Allows for the development of tailored solutions using Python and \\nother frameworks, which may be more suitable for certain tasks.\\n• Spreads the risks by not relying solely on a single vendor, allowing for \\nadaptability in case market dynamics change and ensuring more \\nresilience against vendor-related issues.\\nCons:\\n• More systems to manage can complicate the architecture and require \\nmore robust governance and oversight.\\n• Extra integration effort: Ensuring seamless integration between \\nDatabricks and the Python tech stack could require additional effort \\nand resources.\\n• Teams may require extra needs in expertise in multiple technologies, \\nincreasing the training and onboarding burden.\\nWeighting the pros and cons of the different implementation scenarios\\n#3 Best-of-breed approach\\n (upscaling the existing architecture)\\nIn this best-of-breed architecture, we will leverage the strengths of \\nvarious services and frameworks to build a modular, flexible, and \\nscalable data integration and AI architecture. The architecture will \\nutilize Azure Databricks for data engineering, alongside a containerized \\nPython stack for specialized workloads, enabling seamless integration \\nand optimal resource utilization.\\nPros:\\n• Facilitates the creation of customized solutions optimizing \\nperformance for specific use cases.\\n• More effectively reuse of what we currently already have.\\n• Reduces dependency on a single vendor \\n• Increased adaptability to market changes \\nCons:\\n• Requires a long time to market due to the complexity of managing a \\nhybrid architecture and ensuring all components work together \\neffectively. This is not desired. \\n• This scenario is challenging given the team's lack of experience and \\nthe need to cultivate a strong engineering culture to support such an \\narchitecture.\\nPreferenceConsequences and implications for the existing architecture\\n• Implement a dual-platform strategy for AI development: Use Databricks for data-oriented AI and Zelda IDP for operational AI. Establish strong governance for use case allocation. Develop standardized \\nworkflows and shared libraries.\\n• Enable Databricks as the global data and AI platform: Deployable on both Azure and AWS. Build and deploy AI agents on Databricks, including in Kubernetes.\\n• Create a unified control plane for AI and data services across Azure and AWS: Support provisioning for various use cases. Simplify provisioning and management.\\n• Promote Databricks Lakeflow in favor over Azure Data Factory: Transition period required due to Lakeflow's limited source compatibility.\\n• Enable serverless capabilities for Databricks services: IP address utilization review and possibly restructuring the current VNET architecture to remove implications. \\n• Introduce Databricks Dashboards next to Power BI: Facilitate native analytics workflows with Databricks Dashboards, Databricks Designer and Databricks One. Decrease time-to-insight for users.\\n• Standardize ML lifecycle management using MLflow on Databricks: Shift away from Seldon. Conduct impact assessments for model performance features.\\n• Transition GAIA to Databricks: Utilize Databricks Vector Search and Databricks Apps for future RAG applications. Stop onboarding new GAIA use cases once new architecture is ready.\\n• Implement dual marketplaces for AI and data products: Use Collibra as the data product marketplace. Define a collaborative AI marketplace with engineering community input.\\n• Establish enterprise data quality capability: Select a robust data quality tool that integrates with existing platforms. Automate checks for data accuracy and completeness.\\n• Improve responsible AI governance for agents and hyper-automation: Introduce streamlined onboarding processes. Include responsible AI tooling for compliance and monitoring.\\n• Commit to open standards and specifications: Favor open data and AI standards to avoid vendor lock-in. Ensure standards are reflected in design decisions.\\n• Unify management of structured and unstructured data: Support hybrid data types and cross-environment accessibility.\\n• Allow low-code features within bounded contexts: Utilize embedded agents configured via low-code capabilities offered by various platforms (Salesforce, Microsoft 365, SAP, Adobe, and so on) to \\noptimize development within specific contexts instead of relying on a single low-code platform.\\n• Establish governance and guidelines for standardized frameworks and LLMs: Create comprehensive governance frameworks that define standards for using allowed large language models and other AI \\ntechnologies. Develop guidelines to ensure compliance, quality, and considerations across all AI initiatives.\\n• No AI and Serverless features expansions to existing localized DAP Environments: Teams, such as SIDAP and LPDAP, must plan and execute migration strategies to the new platformized future-state \\nNNDAP & NNDIP architecture for leveraging new features. \\n• Certain products and services — such as DataLab and RAG — will be delivered as managed services. This requires a clear split of responsibilities between EBT and EET. EBT will be responsible for \\ndesigning, delivering, and maintaining these functional services. EET will offer the infrastructure building blocks and operational support layer.\\n• Enable Python as a general-purpose foundation across all data and AI activities: This requires a Python ecosystem, including a package and dependency manager (UV), an IDE, a curated set of libraries \\n(such as FastAPI, FastMCP, Seaborn, Pandas, Dart, Streamlit, PySpark, PyFlink, etc.), integration into Zelda IDP, observability (Logfire), and more.\\n18\\nProposed changesData Intensive Applications Platform\\n19\\nIntegration layer\\nOrchestration\\n(Camunda)\\nData & AI Governance Layer\\nData Management solution\\n(Collibra)\\nAsset Management Solution\\n(Unity Catalog)\\nData Analytics PlatformData Intelligence Platform \\nResponsible AI solution\\n(RfP)\\nMarketplaces\\n(Collibra, Databricks, IDP)\\nEvent & Data Streaming\\n(Confluent Kafka)\\nAPI Management\\n(WSO2  & Azure API Gateway)\\nDQ solution\\n(RfP)\\nTransactional Systems\\nSystems of Record\\n(SAP ERP, SAP FS-PM, Oxygen, TIA, Fineos, etc) \\nDashboarding \\n& Reporting\\nAI Engineering\\nSystems of \\nIntelligence\\nSystems of \\nRecord\\nSystems of Engagement\\n(WebPortals, Mobile Apps, DXP, etc) \\nStream Processing\\n(Apache Flink)\\nData Ingestion Data Platform\\nData Storage\\n(DBX Lakehouse)\\nData Processing\\n(Spark)\\nDatabricks Lakeflow\\nDatabricks \\nAutoloader\\nDatabricks \\nDashboards\\nPowerBI\\nDatabricks SQL\\nAzure Data Factory\\nObservabilityCompute \\nServing Data Serving\\nModel Runtime\\n(Portable Stack)\\nApplication \\nObservability\\n(OTel)\\nApplication Runtime\\n(Potable Stack)\\nFeature Store\\n(tbd)\\nODS \\n(tbd)\\nAgent Observability\\n(MLFlow)\\nAI Agent Runtime\\n(Portable Stack)\\nModel\\nObservability\\n(MLFlow)\\nObservabilityCompute \\nServing Data Serving\\nAI Agent Runtime\\n(Mosaic)\\nAI Vector DB\\n(Mosaic)\\nAI Agent \\nObservability\\n(MLFlow)\\nAI Gateway\\n(Mosaic)\\nAI Agent Evaluation\\n(Mosaic)\\nAI Gateway\\n(MLFlow)\\nStream Processing\\n(Spark)\\nLakehouse \\nMonitoring\\nAgent Integration\\n(MCP, A2A)\\nSystems of \\nIntegration\\nData EngineeringData Intensive Application & AI Engineering\\nAgentic Framework\\n(Mosaic)\\nAgentic Framework  \\n(LangChain)\\nLow Code\\nAgent Bricks\\nAgentic Framework  \\n(LangGraph)\\nHigh CodeLow CodeHigh Code\\nIDP\\n(Zelda)\\nAgentic Framework\\n(Pydantic)\\nLow Code\\n(Dataiku)\\nLow CodeHigh Code\\nVaultspeed Studio\\nDDP\\n(Zelda)\\nTarget operating model\\n2020\\nData & AI Control Tower (Checks & Balances)\\nData & AI Strategy Data & AI Policies & Standards Data & AI Governance Data & AI  Portfolio Mngt\\nCloud Landing Zone’s\\nProductized & Platformized Services\\nPublic Cloud AWS Public Cloud Azure Private Cloud\\nManaged IaaS Platform Portable Platform Cloud Native Platforms Data & AI  Platforms\\n- Productized Databricks- Productized K8s, Productized Camunda\\n- Productized Kafka\\n- Productized Services AWS\\n- Productized Services Azure\\nNNDAP – Data Analytics Platform NNDIP – Data Intelligence Platform\\nDatabricks \\nLakehouse\\nDatabricks SQL Databricks BI\\nDatabricks Lakeflow Databricks \\nMosaic AI\\nMLOps/AIOps \\n(Seldon and Databricks)\\nRAG \\n(GAIA and Databricks)\\nBricks Agents\\nNNDAG – Data & AI Governance Platform\\nDatabricks Unity \\nCatalog\\nAI Governance (tbd) Data Quality (tbd)\\nCollibra\\nGroup IT TBD\\nEET Enterprise Engineering Technology ENABLES the business by delivering the tech & platforms\\nEBT Enterprise Business Technology EMPOWERS the business by delivering knowledge, guidelines, best practices\\nDAI Data & AI Control Tower ENSURES we are doing the right things right21What we propose\\n• Our ambition is to kick-start the development of AI use cases in the third quarter of 2025. Therefore, we propose to identify a small set \\nof diverse use cases (bodily injury, marketing content advisor, and helpdesk assistant) to test the best-of-suite approach and gradually \\ndevelop G-IT services.\\n• For our collaboration with Databricks, we recommend implementing a more structured and controlled collaboration model. We find \\nthis necessary to ensure alignment with our organizational goals and to mitigate potential risks. Databricks is actively engaging with \\nvarious teams within the organization and promoting the rapid adoption of new features. However, the introduction of these new \\nfeatures should be approached with caution, as certain features may incur high costs and/or present significant lock-in risks.\\n• AI is evolving at an incredible speed, and we cannot predict who will emerge as the leader of tomorrow. Therefore, we also \\nrecommend investing in a secondary developer platform that can also address the concerns we have regarding use cases requiring \\ntighter integration with operational systems.\\n• We advocate for horizontal governance across the different tech stacks to ensure that best practices are followed and frameworks are \\nstandardized. Data governance and the associated workflows must be seamlessly integrated into the overall strategic plan.\\n22Appendix\\n23Progressively improving the maturity\\n24\\nBodily injury\\n(Non-life)\\nLegacy x\\n(International) ….\\nGovernance\\nservices L1/L2 L2 L2/3 L3\\nDeveloper \\nservices L1 L1/L2 L2 L3\\nInteraction & \\nintegration L2 L2 L2 L3\\nKnowledge \\nmanagement L2 L2 L2 L3\\nAgentic services L2 L2 L2 L3\\nBase platform L1 L2 L2/3 L3 L3 | Acceptable maturity\\nL2 | Developing maturity\\nL1 | Obsolete / Missing\\nAs-is Plateau 1 Plateau 2 Plateau X\\nQ3’25\\nGovernance services: We propose \\nprioritizing the strengthening of data \\ngovernance as it is essential for ensuring \\ncompliance with regulatory requirements, \\nenhancing data quality, and building trust \\nin our AI systems.\\nScalable platform: We propose making the \\nenhancement of our platform services a \\ntop priority to facilitate seamless \\nintegration and scalability of AI capabilities \\nacross various business units. \\nEnhancing the maturity for the AI-related services at every plateauSystematic approach for delivering AI use cases\\nShowing the architectural needs for the end-to-end process\\nData engineering & API \\nmanagement\\n(if not available)\\nDevelop data pipeline, \\ndesign datasets\\nDesign API endpoint\\nIdentify and register \\napplication\\nData engineering\\nHand-over data script(s) to \\ndata engineering team\\nDevelop data pipeline, \\ndesign datasets\\nRegister new \\ndata products\\nOperationalize\\nRegister final artifacts\\nOptimize scripts (remove \\nhard coded values, etc.)\\nReplace feature \\nengineering code with \\ninput dataset\\nRegister parameters\\nImplement CI/CD process \\n(Go-live)\\nMonitor\\nTechnical logging and \\ntrack behavior\\nEvaluate, bias detection, \\nmodel drift, ECF controls\\nConduct A/B or Multi-\\nArmed Bandit Testing\\nInitiate project\\nDefine business \\nopportunity\\nCreate new DevOps \\nproject\\nInitialize or clone git \\nrepository\\nExperiment #1\\nDefine agentic requirements \\n(runtime + libraries)\\nDevelop Python scripts\\nRequest data products\\nFeature engineering / \\nChunking data\\nTrack experiments\\nRegister and version \\nartifacts (datasets, scripts, \\nmodels)\\nSnapshot code or tag code\\nLog metrics\\nExperiment #n\\nIterate\\nExploratory Phase Operational Phase\\nEnd project\\nUpdate responsible AI \\nassessment\\nInitial responsible AI \\nassessment\\nAssign data ownership \\nat source\\nRegistration/ \\nDecommission\\nUpdate responsible AI use \\ncase end report\\nUpdate model catalog\\nUpdate data catalog\\nGo-live & share \\nknowledge\\nPerform cross-team \\nlearning  \\n… NNDIP\\n… Collibra\\n… Azure DevOps, GitHub\\n… Responsible AI tooling\\n… NNDAP\\nPerform cross-team \\nlearning  \\nSecondary responsible AI \\nassessment\\nSearch marketplace for \\nassets / artifacts\\nRegister (data) products \\nCleanup and archive \\nresources\\nSecurity assessment26\\nUnstructured \\ndocuments, \\nfiles, etc.\\nReal-time\\ndata\\nOperational\\nsystems\\nVector \\nSearch \\nIndex\\nData Lake \\n(Bronze)\\nEmbedding\\nData Lake \\n(Silver)\\nDocument \\nparsing \\nframeworks \\n& LLMs\\nAI \\nGateway\\nConversational \\nExperience\\nUsers\\nExternal foundational \\nmodels, such as OpenAI\\nDatabricks \\nApps\\nStructured \\ndata\\n(Bronze)\\nDatabricks Spark + \\nDeclarative Pipelines\\nStructured \\ndata\\n(Silver)\\nStructured \\ndata\\n(Gold)\\nDatabricks Spark + \\nDeclarative Pipelines\\nRAG & Chatbot pattern (GAIA implementation on Databricks)27\\nOperational\\nsystem #2\\nModel Serving\\nPlayground\\nModels\\nEvaluation Framework\\nTracing\\n Review\\nUnity Catalog\\nFunction\\nAgent Framework\\nExternal foundational \\nmodels, such as OpenAI\\nAI \\nGateway\\nGenAI agent pattern (data-oriented agent with RAG on Databricks)\\nOperational\\nsystem #1\\nVector \\nSearch \\nIndex\\nInput data \\nfor RAG\\nEmbedding\\nMedallion \\nPattern\\n28\\nSelf-service analytics environment on Databricks with Collibra\\nBronze \\ndata\\nDatabricks Spark + \\nDeclarative Pipelines\\nOperational\\nsystems\\nSilver \\ndata\\nGold\\ndata\\nDatabricks Spark + \\nDeclarative Pipelines\\n Databricks Serverless\\nLocal \\nstorage\\nDatabricks Dashboards\\nDatabricks Designer\\nSelf-service environment\\nData Product \\nOwner\\n1. Publish data \\nproduct workflow\\nCollibra\\nData \\nConsumer\\nPublish data product\\n3. Approve \\ndata usage \\nworkflow\\nCreate AD Group for \\ncertified data product\\nApprove data usage\\nLegal / compliance\\n(extra approval?)\\nActive \\nDirectoryautomatic identity management sync\\n1b. sub-step\\n(certified)\\nAdd AD user \\nto AD groupGive AD Group access \\nto Delta tables in UC\\n1\\n2\\n3\\nSailpoint\\nManager \\napproval\\n2. Request data \\nproduct workflow\\nRequest data products\\nData Office\\nCertify data product\\nTables\\nUnity Catalog\\nAccess controlLineage\\nProcess flow:\\n1. Publish Data Products:\\n• Data assets (Delta tables) are \\npublished in Unity Catalog.\\n• Collibra scans the Unity \\nCatalog.\\n• A new data product is created \\nby the data product owner, \\nselecting the relevant data \\nassets.\\n• After publishing, a member of \\nthe data office team must \\nreview and approve the data \\nproduct.\\n• Once approved, the data \\nproduct is certified, a new \\nActive Directory (AD) group will \\nbe created. This group gains \\naccess to the Delta tables \\nassociated with the data \\nproduct.\\n2. Request Data Product:\\n• Users can request data \\nproducts, initiating a new \\nrequest that must be reviewed.\\n3. Approve Data Product:\\n• The data product owner \\nreviews and approves the \\nrequest.\\n• Optionally, approval may also \\nbe required from legal, \\ncompliance, or the data office.\\n• Upon approval(s), the \\nrequested identity will be \\nadded to the AD group, using \\nthe Salespoint process.\\nOperational\\nsystem #1\\nExternal foundational \\nmodels, such as OpenAI\\nGenAI agent pattern (low-latency operational-oriented agents)\\nOperational\\nsystem #2\\nOperational\\nsystem #3\\nDeveloper \\nControl\\nBackstage (Zelda IDP)\\nAzure DevOps AI paved road specification IaC\\nIDE\\nInternal package \\nrepositories with \\nframeworks like Langchain, \\nPydantic, LangGraph, etc.\\n(Nexus)\\nDeveloper \\nControl\\nContainer \\nregister\\nDevOps \\naction\\nKubernetes environment\\nAgentAgentAgent\\nPostgreSQL\\ndatabase\\nResources\\nWeb appALFRED (example of a finance business application on Databricks)\\n30\\nReal-time\\ndata\\nOperational\\nsystems\\nNEF Landing \\nZone\\n(Lakehouse)\\nDatabricks Spark + \\nDeclarative Pipelines\\nEnriched \\nZone\\n(Lakehouse)\\nHarmonized \\nzone\\n(Lakehouse)\\nDatabricks Spark + \\nDeclarative Pipelines\\nDatabricks \\nApps\\n(web app)\\nLanding \\narea\\nAgent\\n(Validate and \\ntransform)Users \\nuploading \\ninput files\\nDatabricks Spark + \\nDeclarative Pipelines\\nOperational \\ndata mall\\n(Lakebase)\\nCollibra\\nLineage Glossary DQ Results\\nRegulatory requirements\\nRDM data\\nQRT\\nECAPS\\nECAPS\\nAgentic layer\\nVector \\nSearch \\nIndex\\nEmbedding\\nAgent\\nAgent\\nAgent\\nAgent\\nDatabricks \\nApps\\n(web app)\\n Users \\ndownloading \\ndata\\nDatabricks SQL\\nDDA \\nAgreements\\nDDA \\nAgreements\\nNEF Interface \\nstandards\\nLDM\\nMetadata management\\nNEF Mapper\\nRDM Tool\\nData products\\nMarkets \\ndata\\nMEDM / MIP\\nNEF \\nValidation\\nCalculation \\nengines\\nRegulatory \\nchatbot\\nregulatory \\ncompliance \\nmonitoring \\nagents\\nFinance \\ncontrolling \\nagent\\nNEF Mapping \\nvalidation agentWhat International proposes\\n31\\nTransitioning from a best-of-breed approach to a best-of-suite approach\\nThe aim is to utilize more managed services and eliminate external \\nresources, facilitating easier integration\\n1\\nUsing fewer components simplifies the engineering process and reduces \\nthe likelihood of bugs\\n2\\nThe local DAP initially began using Databricks as a processing engine, \\nhowever, with the availability of more out-of-the-box components, we are \\ntransitioning to a complete Lakehouse implementation on Databricks\\n3\\nDatabricks is a leader in Lakehouse architecture and continues to \\nintroduce new features\\n4\\nAzure\\nAzure Services Databricks\\nCustom \\ndeveloped \\ncomponents\\nAzure\\nAzure \\nServices Databricks\\nCustom \\ndeveloped \\ncomponents\\nBEST OF \\nBREED\\nBEST OF \\nSUITE32\\nSources Use CaseTransform and ProcessExtract / Ingest\\nStructured data\\nSemi-structured data\\nUnstructured data\\nOutput\\nData\\nData Processing\\nAutomation\\nDelta Live Tables\\nOn-demand/Analyses processing\\nBatch\\nData Intelligence Engine\\nSRC\\nSRC\\nCRM\\nJSON\\nStreaming\\nData Management\\nServerless\\nLakehouse federation\\nJob Clusters Workflow\\nPredictive IO\\nAzure DevOps DataOps\\nAll-purpose \\nCluster\\nServerless SQL \\nwarehouse \\nPredictive optimization\\nUnity Catalog\\nODS\\nCollibra\\nData Architecture\\nTalend\\nSilverBronze Gold\\nData Platform Governance\\nRow/Column level security Catalog & Lineage\\nVolumes Delta Tables\\nData Exchange\\nData Platform\\nStructured data\\nSRC\\nSRC\\nCRM\\nDelta Sharing\\nAssistant\\nAccess Control\\nMLOps\\nStreaming\\nEvent Hub\\nSpark Streams\\nCDC\\nDatabricks \\nconnectors\\nStorage ADLS Gen 2\\n Azure SQL DB\\nData Quality in Databricks\\nAPI\\nAPIM\\nLLMops\\nInternational Data Platform Capabilities – Target Reference Implementation\\nModern Data Lakehouse build on top of Azure Databricks should implement the following services\\n* Databricks Lakeflow Connectors show considerable potential, though some connectors remain in the preview stage and have yet to be fully tested.\\nFivetran connectors in \\nDatabricks\\nJDBC connectors\\nData Science & Machine Learning\\nMosaic AI AutoMLMLflow\\nSelf-service & Sandboxing\\nLAB environmentCloud sandboxPower Apps\\nData Analysis & Investigation\\nMS Fabric or Databricks \\nNotebooks & Dashboards\\nAI/BI Genie space\\nReporting\\nDatabricks Dashboards\\nMS Fabric\\nCentralized Use Cases\\nNNI DIP\\nNNDIP (Seldon)\\nGroup reporting\\nNN  GAIA\\nAutoloader\\n33\\nKey concepts\\n• Mosaic AI converts your Databricks platform into a comprehensive AI agent factory, allowing you to design and \\ndeploy AI agents using your existing data infrastructure without the need for additional tools. In contrast, Azure AI \\nFoundry is more complex to manage, less integrated, and requires additional Azure services.\\n• Unity Catalog functions as the central security feature, facilitating detailed governance across both AI components \\nand data resources. It offers a unified management system that accommodates different permission rules for each \\nresource type. It is important to overwatch which agent has access to which data. \\n• AI agents utilize LLM engines to effectively handle natural language, engage in reasoning, and produce responses \\nakin to human interaction. The MLflow AI Gateway facilitates seamless integration with foundation models from \\nvarious providers, including OpenAI, Anthropic, Google, as well as offering the option to employ open-source \\nmodels. Remaining LLM agnostic is a key feature that enables swift adaptation in this rapidly changing field.\\n• These agents integrate with your business systems via specialized tools (such as database connectors, API clients, \\nand semantic search), while upholding strict enterprise security standards enforced by Unity Catalog. To simplify \\naccess to business system, Model Context Protocol (MCP) establishes standardized, secure connections between \\nyour AI agents and enterprise data sources, reducing integration complexity while maintaining stringent security \\nmeasures. \\n• Additionally, Databricks enables real-time ingestion of event streams (such as Kafka or Event Hubs) directly into \\nDelta lake with sub-second latency, ensuring AI agents have access to recent data for downstream applications.\\n• Mosaic AI is Agentic AI agnostic, all the major frameworks such as Langchain, LangGraph, LlamaIndex, PydenticAI, \\nCrewAI are supported by Databricks.\\n• Once operational, Mosaic AI Agent Evaluation continuously assesses quality, cost, and performance using \\nspecialized LLM evaluators, offering metrics and insights to enhance agents' performance over time.\\n• Maintain a global component library in Databricks Unity Catalog for standardized resources.\\nRisks\\n• Technology Maturity: The Mosaic AI Gateway and Agent Framework were launched in mid-2024. As they are still in \\nthe Public Preview stage, along with the support for leading frameworks LangChain and LangGraph, users should be \\naware of potential ongoing development and adjustments.\\n• Agentic Protocols (MCP, A2A): While these protocols are currently being introduced, they have yet to become \\nindustry standards. This situation could lead to significant changes in the future.\\nDownstream \\nApplication \\nIntegration\\nMosaic AI Gateway\\nUnity Catalog\\nAPIs\\nModel Context \\nProtocol (MCP)\\nEvents\\nNNI DAP (L1+L2)\\nL3\\nData Products\\n(ICDM, Feature \\nStore)\\nData Lakehouse \\nLayers \\n(Bronze / Silver / \\nGold)\\nL2\\nMultitenant\\nServing Metadata\\n• Human \\nfeedback\\n• Metric tables\\n• Inference \\ntables\\nFoundation \\nModels\\nMosaic AI Vector \\nDatabase\\nMosaic AI Agent \\nFramework\\nMosaic AI \\nAgent \\nEvaluation\\nLakehouse\\nMonitoring\\nMLflow AI \\nGateway\\nMLflow \\nTracking \\nServer\\nAgent to Agent \\nProtocol (A2A)\\nDatabase links\\nNNI Mediation \\nLayer\\nProposed International Agentic AI Architecture\\nThe agentic AI architecture primary uses Mosaic AI\\n33\\nNNI Agentic AI Platform\\nSelf-Service Front-End\\nfor Citizen deployment (NNI BUs)\\n(Step 2)\\nProtocol \\nAbstraction LayerImplications for other Business Units\\n34\\nThe scope of international mostly touches the analytical side of the architecture\\nConversational layer\\n(interfaces for web, speech, search, instructions, etc.)\\nAgentic layer\\n(query resources, execute tasks, perform reflections, orchestrate, etc.)\\nContext-aware layer\\nAPIs\\nOperational systems\\nContext-aware layer\\nAnalytical platforms\\n(structured + unstructured)\\nSQLGenAI\\nML Runtime\\nML\\nGenAI Runtime\\nReports\\nScope International\\nImplications for other business units\\n• With international operations primarily utilizing Azure \\nand other business units using AWS, a coordinated \\nstrategy is needed \\n• Mitigation of lock-in and cost risks, particularly \\nconcerning Databricks, are needed.\\n• A systematic approach to governance is essential, \\nincluding the publication of use cases, identification of \\ndata ownership, and the development of data products.\\n• There is a need for improved integration with the \\napplication and operational-oriented arch types, \\nespecially for low-latency use cases running on AWS.\\n• Integration patterns with data-intensive applications will \\nbe crucial for maximizing the effectiveness of AI \\ninitiatives.Storage ADLS Gen 2\\n Azure SQL DB\\n PostgreSQL\\n(AWS) \\n35\\nSources Data usageTransform and ProcessExtract / Ingest\\nStructured data\\nSemi-structured data\\nUnstructured data\\nOutput\\nData\\nData Processing\\nAutomation\\nDelta Live Tables\\nOn-demand/Analyses processing\\nBatch\\nData Intelligence Engine\\nSRC\\nSRC\\nCRM\\nJSON\\nStreaming\\nData Management\\nServerless\\nLakehouse federation\\nJob Clusters Workflow\\nPredictive IO\\nAll-purpose \\nCluster\\nServerless SQL \\nwarehouse \\nPredictive optimization\\nUnity Catalog ODS\\nCollibra\\nData Architecture\\nTalend\\nSilverBronze Gold\\nData Platform Governance\\nRow/Column level security Catalog & Lineage\\nVolumes Delta Tables\\nData Exchange\\nData Platform\\nStructured data\\nSRC\\nSRC\\nCRM\\nDelta Sharing\\nCode Assistant\\nAccess Control\\nStreaming\\nConfluent + \\nEvent Hub\\nSpark Structured \\nStreaming\\nCDC\\nDatabricks \\nconnectors\\nData Quality in Databricks\\nAPI\\nAPIM\\nData Platform Capabilities – Target Reference Implementation\\nProposed refinements for the reference architecture of International\\nFivetran connectors in \\nDatabricks\\nJDBC connectors\\nMachine Learning\\nFeature store AutoMLMLflow\\nSelf-service & Sandboxing\\nLAB environmentCloud sandboxPower Apps\\nData Analysis & Investigation\\nMS Fabric or Databricks \\nNotebooks & Dashboards\\nAI/BI Genie space\\nReporting\\nDatabricks Dashboards\\nPower BI (Fabric)\\nGenerative AI\\nLow-code / no-code\\nAgent to Agent Protocol\\nFoundational models\\nAI Vector Database\\nAutoloader\\nApplication \\nintegration\\nAPI Gateway\\nEvents\\nUnstructured data processing\\nMDM (master & reference \\ndata management) Responsible AI governance\\nMetadata-driven \\nAnonymization\\nCDM\\nCollibra –> Databricks \\nintegration\\nMCP\\nAzure DevOps DataOps MLOpsCI/CD Deployment Framework LLMOps\\nData-intensive \\napplications\\nContainer \\nservices\\nIntegrationProposed renewed Data & AI capability model \\n36\\nData governance\\nStrategy and use case \\nmanagement\\nData quality\\nData quality profiling \\nMaster Data \\nManagement\\nReference data \\nmanagement\\nData modelling and \\ndesign\\nData value \\nmanagement\\nData ownership \\nmanagement\\nData principles & \\npolicy management\\nData awareness\\nData literacy\\nData ethics\\nData quality \\nmonitoring\\nData issue and \\nremediation \\nmanagement\\nDQ dashboarding\\nMaster data \\nmanagement\\n Logical data modelling\\nPhysical data \\nmodelling\\nConceptual data \\nmodelling\\nCitizen data scientist \\ndiscovery, experiment \\nand enablement (also \\nincludes no-/low-\\ncode GenAI\\nSelf-service business \\nintelligence\\nManaged business \\nintelligence\\nML & Agentic \\ndevelopment\\nML & Agentic training \\nand fine-tuning\\nML & Agentic \\nexperiment tracking \\nand model \\nmanagement \\nML & Agentic \\ndeployment\\nML & Agentic \\nobservability\\nML & Agentic \\ninference \\nmanagement\\nData source \\nmanagement\\nList of golden sources \\nmanagement\\nDocument & content \\nmanagement\\nData integration and \\ninteroperability\\nData sharing \\nagreement \\nmanagement\\nData extraction and \\ningestion\\nData transforming\\nData orchestration \\nand scheduling\\nData publication and \\nexchange\\nData pipeline \\nmonitoring and \\nobservability\\nMetadata \\nmanagement\\nLineage management\\nMetamodel (and \\ngraph relationship) \\nmanagement\\nGlossary and context \\nmanagementData operations \\nmanagement\\nResilience & recovery\\nArchiving\\nData life cycle \\nmanagement\\nData security\\nData anonymization, \\npseudonymization \\nand synthetic data \\nmanagement\\nLabel, tag and \\nclassification \\nmanagement\\nData access \\nmanagement\\nConsent management\\nData platform \\nmanagement\\nPlatform observability \\nand monitoring\\nPlatform integration \\ntesting\\n(Business) rule \\nmanagement\\nData product and \\nasset management\\nCompliance \\nmanagement\\nData streaming & \\nStreaming analytics\\nConversational and \\nchatbot management\\nVector and \\nembedding \\nmanagement\\nData hub function\\nOperational data \\nprocessing (ODS)Best-of-suite versus best-of-breed considerations\\n• Databricks is multi-cloud. \\n• Databricks uses open-source standards, like Delta, Spark, MLflow, Unity \\nCatalog, etc.\\n• Databricks is tightly integrated into the Azure ecosystem and therefore \\nfacilitating strong security (usage of service principles, AD groups, key vault, \\netc.). This integration isn’t present in any (open-source) alternatives. \\n• Integrating lots of components yourself is a daunting task.\\n• Often lower total ownership costs due to bundling of services.\\n• Uniform user experience and interface. Faster time to market.\\n• Easier to hold one vendor accountable for support and updates.\\n• Dependency on a single vendor can pose risks. Must be mitigated by not \\nusing too much high-risk lock-in services.\\n• Not all necessary functionalities may be available, so still some lightweight \\nintegration might be required. This should be weighted carefully against \\nextra effort and costs.\\n37\\nDatabricks Data & AI Architecture\\nML + Data-oriented agents\\nApplication-oriented agents\\nProposed future-state reference architecture for data & AI – scenario #1\\n38\\nTwo run-time stacks for AI Agents: Kubernetes (K8s) and Databricks\\nIntegration layer\\nAPI Gateway (WSO2)\\n Event Streaming (Kafka + Flink)\\nOperational Data Stores (PostgreSQL, Azure SQL, Lakebase)\\nGovernance services\\nData Intelligence Platform (Databricks)\\nDatabricks \\nApps\\nFoundation \\nmodel APIs\\nMlflow\\n(model governance)\\nAI \\nGateway\\nAutoML\\nAI \\nPlayground\\nMosaic AI Agent Framework\\n(tooling for development, build and deploy)\\nModel \\nServing\\nVector \\nSearch\\nAgent Bricks\\nFoundational Models, such \\nas OpenAI & Anthropic\\nSource system\\nData layer\\nApplication \\nlayer\\nInference runtime \\nenvironment\\n(Kubernetes)\\nAgents & model \\ninstances\\nMonitoring & \\nmanagementZelda IDP\\nLightweight Python \\ndev environment for \\ndeveloping \\noperational-aligned \\nagents\\nBase images with \\nPython libraries\\nAI agent \\ndevelopment, \\ntesting, deployment\\nTracking \\nand tracing\\nData Analytics platform (Databricks)\\nLakehouse \\n(Medallion design with Delta \\n+ Iceberg for both structured \\nand unstructured data)\\nSpark (data \\nprocessing & \\nML)\\nDatabricks \\nSQL\\nDatabricks \\nDashboards\\nPowerBI\\nDatabricks \\nLakeflow\\nLow-code*\\nLow-code\\n* Recognize that various platforms (e.g., Salesforce, SAP, Adobe, Databricks, Microsoft 365 & Teams, Collibra) are increasingly incorporating pay-per-use \\nlow-code features tailored to their specific services. Overseeing this trend, we should utilize the embedded agents (configured via low-code features) \\noffered by our chosen platforms to optimize development within specific contexts. \\nLakebase\\nCollibra\\n Unity Catalog\\nResponsible AI (TBD)\\nAI Collaborative Marketplace (TBD)\\nData Quality (TBD)\\nLow-code\\nExtra embedding, RAG, \\nMCP services, etc.\\nLow-code\\nAI Foundry voorstellen, voor \\nwebrtc, fine-tuning, complexe \\nsecurity (agent ID)\\nAgentic development: Copilot Studio + \\nVSCode\\nCopilot Studio + \\nVSCode voor AI \\ndevelopment GAIA vervangt door \\nCopilot Studio + \\nRAG (AI Search)\\n• Low-latency voice live APIs. Sanne GPT? Web RTC. OCR, \\nGaat niet binnen Databricks werken.\\n• Fine-tuning. Microsoft heeft meerdere fine-tune opties. \\nOnwaarschijnlijk dat dit binnen Databricks werkt.\\n• Kosten zorg. Databricks komt met veel meer overhead en \\nextra DBU kosten.\\n• Complexe security (Microsoft AI safetify framework)\\n• Agent ID (active directory, passthrough identity waarmee \\nachterliggende agents weer data kunnen ophalen).\\n• Copilot Studio ook binnen deze scope?\\n• Als vervanging van GAIA, en dan scope iets oprekken naar \\nTeams, SharePoint, Microsoft 365, etc.\\n• Open Telemetry ondersteuning in Databricks\\n• AI Foundry gaat straks samen op met Azure ML Services.\\nAgent \\nstore\\nEmployee productivity\\n\\nWith this context, please chat with the user.Crucial Rules:- You must allow the user to make revisions on the ADR you write.- You must ask the user whether the ADR is ready to be handed off, and if so, you must hand it off to the store_adr tool.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def store_adr(name, content):\n",
    "    # Define the file name and content  \n",
    "    file_name = name\n",
    "    \n",
    "    # Open the file in write mode ('w')  \n",
    "    with open('files\\\\' + file_name + '.md', 'w') as file:  \n",
    "        file.write(content)\n",
    "    return {\"recorded\": \"ok\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionTool(name='store_adr', description='', params_json_schema={'properties': {'name': {'title': 'Name'}, 'content': {'title': 'Content'}}, 'required': ['name', 'content'], 'title': 'store_adr_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B59A791940>, strict_json_schema=True, is_enabled=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at it\n",
    "store_adr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [store_adr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionTool(name='store_adr', description='', params_json_schema={'properties': {'name': {'title': 'Name'}, 'content': {'title': 'Content'}}, 'required': ['name', 'content'], 'title': 'store_adr_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B59A791940>, strict_json_schema=True, is_enabled=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(message, history):\n",
    "    try:\n",
    "        # Make an agent with name, instructions, model\n",
    "        agent = Agent(\n",
    "            name=\"ADR Writer\", \n",
    "            instructions=system_prompt,\n",
    "            model=OpenAIChatCompletionsModel(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                openai_client=openai_client\n",
    "            ),\n",
    "            tools=tools\n",
    "        )\n",
    "        result = await Runner.run(agent, message)\n",
    "        print(result)\n",
    "        return result.final_output\n",
    "    except OpenAIError as e:\n",
    "        print(f\"OpenAI API Error: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
